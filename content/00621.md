---
title: Tanzu Kubernetes Grid 1.1ã‚’vSphere 6.7ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ¡ãƒ¢
tags: ["Kubernetes", "vSphere", "TKG", "Tanzu", "Cluster API", "MetalLB"]
categories: ["Dev", "CaaS", "Kubernetes", "TKG", "vSphere"]
---

æœ¬è¨˜äº‹ã¯ã€[Tanzu Kubernetes Grid (TKG)](https://docs.vmware.com/jp/VMware-Tanzu-Kubernetes-Grid/index.html) 1.1.3ã‚’vSphereã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ¡ãƒ¢ã§ã™ã€‚

> [Tanzu Kubernetes Grid 1.1ã‚’AWSã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ¡ãƒ¢](https://blog.ik.am/entries/543)ã®vSphereç‰ˆã§ã™ã€‚

TKGã¯[Cluster API](https://cluster-api.sigs.k8s.io)(`clusterctl`)ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã®ã‚ˆã†ãªãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã§ã™ã€‚

Cluster APIã«ã¯

* Management Cluster
* Workload Cluster

ã¨ã„ã†2ç¨®é¡ã®Kubernetes ClusterãŒã‚ã‚Šã¾ã™ã€‚

Workload Clusterã¯Management Clusterã«ã‚ˆã£ã¦ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’ç®¡ç†ã•ã‚Œã¾ã™ã€‚<br>
ã¤ã¾ã‚Šã€Kubernetesã§ç®¡ç†ã•ã‚Œã‚‹Kubernetes Clusterã§ã™ã€‚

ã§ã¯ã€Kubernetesã‚’ç®¡ç†ã™ã‚‹Kubernetesã¯èª°ãŒç®¡ç†ã™ã‚‹ã®ã§ã—ã‚‡ã†ã‹?

...Kubernetesã§ã™ğŸ˜… <br>
TKGã§ã¯Management Clusterã¯[Kind](https://kind.sigs.k8s.io)ã«ã‚ˆã£ã¦bootstrapã•ã‚Œã¾ã™ã€‚

ã¤ã¾ã‚Š Kind -> Management Cluster -> Workload Clusterã¨ã„ã†è¦ªå­é–¢ä¿‚ã§Kubernetes ClusterãŒç®¡ç†ã•ã‚Œã‚‹ãŸã‚ã€
Cluster APIã¯æ¬¡ã®ã‚ˆã†ãªäº€ã®ãƒ­ã‚´ã«ãªã£ã¦ã„ã¾ã™ã€‚

<img src="https://user-images.githubusercontent.com/106908/90330477-e7588900-dfe7-11ea-86a6-91533b0b2fa1.png" width="200" />

Cluster APIã«ã¤ã„ã¦ã‚ˆã‚ŠçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€â†“ã®è³‡æ–™ã‚„

* https://www.cncf.io/wp-content/uploads/2020/06/Cluster-API-CNCF-Webinar-20200611.pdf

æ¬¡ã®KubeAcademyã®å‹•ç”»ã‚’è¦‹ã‚‹ã¨è‰¯ã„ã§ã™ã€‚

* https://kube.academy/lessons/bootstrapping-cluster-api-part-1-concepts-components-and-terminology
* https://kube.academy/lessons/bootstrapping-cluster-api-part-2-creating-a-cluster-on-aws-with-cluster-api

> vSphere 7ã®å ´åˆã¯[TKG Service](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-7E00E7C2-D1A1-4F7D-9110-620F30C02547.html)ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§ã€Management Clusterã«ç›¸å½“ã™ã‚‹ã€Supervisor ClusterãŒæ‰‹ã«å…¥ã‚‹ã®ã§ã€ã“ã®æ‰‹é †ã§TKGã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

**Table of contents**
<!-- toc -->

### CLIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

[https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121](https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121)

ã‹ã‚‰

- VMware Tanzu Kubernetes Grid CLI for Mac

ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¬¡ã®ã‚ˆã†ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

```
gunzip tkg-darwin-amd64-v1.1.3-vmware.1.gz

mv tkg-darwin-amd64-v1.1.3-vmware.1 /usr/local/bin/tkg

chmod +x /usr/local/bin/tkg
```

ãã®ä»–ã€DockerãŒå¿…è¦ã§ã™ã€‚

æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™ã€‚

```
$ tkg version
Client:
	Version: v1.1.3
	Git commit: 0e8e58f3363a1d4b4063b9641f44a3172f6ff406
```

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

```
tkg get management-cluster
```

æ¬¡ã®ã‚ˆã†ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒä½œæˆã—ã¾ã™ã€‚

```
$ find ~/.tkg 
/Users/toshiaki/.tkg
/Users/toshiaki/.tkg/bom
/Users/toshiaki/.tkg/bom/bom-1.1.2+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-tkg-1.0.0.yaml
/Users/toshiaki/.tkg/bom/bom-1.17.6+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-1.1.0+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-1.17.9+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-1.1.3+vmware.1.yaml
/Users/toshiaki/.tkg/providers
/Users/toshiaki/.tkg/providers/control-plane-kubeadm
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.6
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.6/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.5
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.5/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.3
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.3/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/providers.md5sum
/Users/toshiaki/.tkg/providers/infrastructure-vsphere
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/cluster-api
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.6
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.6/core-components.yaml
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.5
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.5/core-components.yaml
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.3
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.3/core-components.yaml
/Users/toshiaki/.tkg/providers/config.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.6
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.6/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.5
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.5/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.3
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.3/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0/cluster-template-prod.yaml
/Users/toshiaki/.tkg/config.yaml
```

### vCenterã«OVAãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

[https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121](https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121)

ã‹ã‚‰

- Photon v3 Kubernetes v1.18.6 OVA
- Photon v3 capv haproxy v1.2.4 OVA

ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`~/Downloads`ã«ä¿å­˜ã—ã¾ã™ã€‚

TKGã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ç’°å¢ƒã®æƒ…å ±ã‚’æ¬¡ã®ä¾‹ã®ã‚ˆã†ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å…ˆã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯**DHCPãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™**ã€‚

```
cat <<EOF > tkg-env.sh
export GOVC_URL=vcsa-01.haas-409.pez.vmware.com
export GOVC_USERNAME=administrator@vsphere.local
export GOVC_PASSWORD=VMware1!
export GOVC_DATACENTER=Datacenter
export GOVC_NETWORK=Extra
export GOVC_DATASTORE=LUN01
export GOVC_RESOURCE_POOL=/Datacenter/host/Cluster/Resources/tkg
export GOVC_INSECURE=1
export TEMPLATE_FOLDER=/Datacenter/vm/tkg
EOF
```

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§OVAãƒ•ã‚¡ã‚¤ãƒ«ã‚’vCenterã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```
source tkg-env.sh
govc pool.create /Datacenter/host/Cluster/Resources/tkg
govc folder.create /Datacenter/vm/tkg

govc import.ova -folder $TEMPLATE_FOLDER ~/Downloads/photon-3-kube-v1.18.6_vmware.1.ova
govc import.ova -folder $TEMPLATE_FOLDER ~/Downloads/photon-3-haproxy-v1.2.4-vmware.1.ova

govc vm.markastemplate $TEMPLATE_FOLDER/photon-3-kube-v1.18.6
govc vm.markastemplate $TEMPLATE_FOLDER/photon-3-haproxy-v1.2.4
```

### SSH Keyã®ä½œæˆ

TKGã§ä½œæˆã•ã‚Œã‚‹VMç”¨ã®SSH Keyã‚’ä½œæˆã—ã¾ã™ã€‚

```
export AWS_SSH_KEY_NAME=tkg-sandbox
aws ec2 create-key-pair --key-name tkg-sandbox --output json | jq .KeyMaterial -r > tkg-sandbox.pem
```

```
ssh-keygen -t rsa -b 4096 -f ~/.ssh/tkg
```

ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã¯ç©ºã«ã—ã¦ãã ã•ã„ã€‚

### Management Clusterã‚’AWSã«ä½œæˆ

TKGã§Workload Clusterã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ã€Management Clusterã‚’ä½œæˆã—ã¾ã™ã€‚

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§vCenterã®æƒ…å ±ã‚’Management Clusterç”¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«(`~/.tkg/config.yaml`)ã«è¿½è¨˜ã—ã¾ã™ã€‚

```
source tkg-env.sh
cat <<EOF >> ~/.tkg/config.yaml
VSPHERE_SERVER: ${GOVC_URL}
VSPHERE_USERNAME: ${GOVC_USERNAME}
VSPHERE_PASSWORD: ${GOVC_PASSWORD}
VSPHERE_DATACENTER: ${GOVC_DATACENTER}
VSPHERE_DATASTORE: ${GOVC_DATACENTER}/datastore/${GOVC_DATASTORE}
VSPHERE_NETWORK: ${GOVC_NETWORK}
SERVICE_CIDR: 100.64.0.0/13
CLUSTER_CIDR: 100.96.0.0/11
VSPHERE_RESOURCE_POOL: ${GOVC_RESOURCE_POOL}
VSPHERE_FOLDER: ${TEMPLATE_FOLDER}
VSPHERE_CONTROL_PLANE_NUM_CPUS: "2"
VSPHERE_CONTROL_PLANE_MEM_MIB: "4096"
VSPHERE_CONTROL_PLANE_DISK_GIB: "40"
VSPHERE_WORKER_NUM_CPUS: "2"
VSPHERE_WORKER_MEM_MIB: "4096"
VSPHERE_WORKER_DISK_GIB: "40"
VSPHERE_HA_PROXY_NUM_CPUS: "2"
VSPHERE_HA_PROXY_MEM_MIB: "4096"
VSPHERE_HA_PROXY_DISK_GIB: "40"
VSPHERE_HAPROXY_TEMPLATE: ${TEMPLATE_FOLDER}/photon-3-haproxy-v1.2.4
VSPHERE_SSH_AUTHORIZED_KEY: $(cat ~/.ssh/tkg.pub)
EOF
```

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§Management Clusterã‚’ä½œæˆã—ã¾ã™ã€‚Control Planeã®å°æ•°ã¯`dev` planãŒ1å°ã€`prod`ãŒ3å°ã«ãªã‚Šã¾ã™ã€‚

```
tkg init --infrastructure vsphere --name tkg-sandbox --plan dev
```

> `tkg init`ã§Kindã‚¯ãƒ©ã‚¹ã‚¿ä½œæˆã€`clusterctl init`ã€`clusterclt config`ã€`kubectl apply`ç›¸å½“ã®å‡¦ç†ãŒè¡Œã‚ã‚Œã¾ã™ã€‚

æ¬¡ã®ã‚ˆã†ãªãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

```
Logs of the command execution can also be found at: /var/folders/76/vg4pyy253pbgwzncmx2mb1gh0000gq/T/tkg-20200922T194233610375911.log

Validating the pre-requisites...

Setting up management cluster...
Validating configuration...
Using infrastructure provider vsphere:v0.6.6
Generating cluster configuration...
Setting up bootstrapper...
Bootstrapper created. Kubeconfig: /Users/toshiaki/.kube-tkg/tmp/config_q0ppw4E1
Installing providers on bootstrapper...
Fetching providers
Installing cert-manager
Waiting for cert-manager to be available...
Installing Provider="cluster-api" Version="v0.3.6" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-vsphere" Version="v0.6.6" TargetNamespace="capv-system"
Start creating management cluster...
Saving management cluster kuebconfig into /Users/toshiaki/.kube/config
Installing providers on management cluster...
Fetching providers
Installing cert-manager
Waiting for cert-manager to be available...
Installing Provider="cluster-api" Version="v0.3.6" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-vsphere" Version="v0.6.6" TargetNamespace="capv-system"
Waiting for the management cluster to get ready for move...
Moving all Cluster API objects from bootstrap cluster to management cluster...
Performing move...
Discovering Cluster API objects
Moving Cluster API objects Clusters=1
Creating objects in the target cluster
Deleting objects from the source cluster
Context set for management cluster tkg-sandbox as 'tkg-sandbox-admin@tkg-sandbox'.

Management cluster created!


You can now create your first workload cluster by running the following:

  tkg create cluster [name] --kubernetes-version=[version] --plan=[plan]
```

vSphereä¸Šã«ã¯æ¬¡ã®VMãŒä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/93874343-2f1caf80-fd0e-11ea-879b-259985e42603.png)

`***-lb`ã¯Master VMã«å¯¾ã™ã‚‹LBã§ã€å®Ÿä½“ã¯HA Proxyã§ã™ã€‚

`tkg get management-cluster`ã‚³ãƒãƒ³ãƒ‰ã§Management Clusterä¸€è¦§ã‚’å–å¾—ã§ãã¾ã™ã€‚

```
$ tkg get management-cluster
 MANAGEMENT-CLUSTER-NAME  CONTEXT-NAME                  
 tkg-sandbox *            tkg-sandbox-admin@tkg-sandbox 
```

`tkg get cluster --include-management-cluster`ã‚³ãƒãƒ³ãƒ‰ã§Workload ClusteråŠã³Management Clusterä¸€è¦§ã‚’å–å¾—ã§ãã¾ã™ã€‚

```
$ tkg get cluster --include-management-cluster
   NAME         NAMESPACE   STATUS   CONTROLPLANE  WORKERS  KUBERNETES       
   tkg-sandbox  tkg-system  running  1/1           1/1      v1.18.6+vmware.1
```

Management Clusterã«`kubectl`ã‚³ãƒãƒ³ãƒ‰ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã¿ã¾ã™ã€‚

```
$ kubectl config use-context tkg-sandbox-admin@tkg-sandbox
$ kubectl cluster-info
Kubernetes master is running at https://10.213.173.208:6443
KubeDNS is running at https://10.213.173.208:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
$ kubectl get node -o wide
NAME                                STATUS   ROLES    AGE   VERSION            INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                 KERNEL-VERSION   CONTAINER-RUNTIME
tkg-sandbox-control-plane-p276c     Ready    master   14m   v1.18.6+vmware.1   10.213.173.209   10.213.173.209   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
tkg-sandbox-md-0-6498fc8bd4-k59gr   Ready    <none>   11m   v1.18.6+vmware.1   10.213.173.210   10.213.173.210   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4

$ kubectl get pod -A -o wide
NAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE     IP               NODE                                NOMINATED NODE   READINESS GATES
capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-6857dfc668-6q9c2       2/2     Running   0          9m37s   100.99.203.8     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-85f4885cf5-5m5qj   2/2     Running   0          9m17s   100.114.41.134   tkg-sandbox-control-plane-p276c     <none>           <none>
capi-system                         capi-controller-manager-5df8c8fb59-9r9k2                         2/2     Running   0          9m59s   100.99.203.6     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capi-controller-manager-7d8d9b87b8-ngwfc                         2/2     Running   0          10m     100.99.203.5     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capi-kubeadm-bootstrap-controller-manager-dff99d987-48fh6        2/2     Running   1          9m49s   100.99.203.7     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capi-kubeadm-control-plane-controller-manager-6cc995dd6c-zpj7g   2/2     Running   0          9m30s   100.99.203.9     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capv-controller-manager-bfb88d6df-zkshm                          2/2     Running   1          9m5s    100.114.41.135   tkg-sandbox-control-plane-p276c     <none>           <none>
capv-system                         capv-controller-manager-7844b47584-dslbv                         2/2     Running   0          8m51s   100.114.41.136   tkg-sandbox-control-plane-p276c     <none>           <none>
cert-manager                        cert-manager-b56b4dc78-58tkh                                     1/1     Running   0          11m     100.99.203.3     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
cert-manager                        cert-manager-cainjector-6b54f84d85-cs6k7                         1/1     Running   0          11m     100.99.203.2     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
cert-manager                        cert-manager-webhook-6fbc6d7449-j2g4v                            1/1     Running   0          11m     100.99.203.4     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         calico-kube-controllers-54fd4b48dd-2k8d7                         1/1     Running   0          12m     100.114.41.133   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         calico-node-cx76v                                                1/1     Running   0          11m     10.213.173.210   tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         calico-node-kx2wm                                                1/1     Running   0          12m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         coredns-5cf78cdcc-58ssl                                          1/1     Running   0          13m     100.114.41.130   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         coredns-5cf78cdcc-z86j8                                          1/1     Running   0          13m     100.114.41.129   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         etcd-tkg-sandbox-control-plane-p276c                             1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-apiserver-tkg-sandbox-control-plane-p276c                   1/1     Running   0          12m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-controller-manager-tkg-sandbox-control-plane-p276c          1/1     Running   1          12m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-proxy-h2c6p                                                 1/1     Running   0          11m     10.213.173.210   tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         kube-proxy-zptl6                                                 1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-scheduler-tkg-sandbox-control-plane-p276c                   1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         vsphere-cloud-controller-manager-xb5gb                           1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         vsphere-csi-controller-8c9b98f7f-jn4gx                           5/5     Running   0          13m     100.114.41.132   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         vsphere-csi-node-24mjf                                           3/3     Running   0          11m     100.99.203.1     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         vsphere-csi-node-g8pcq                                           3/3     Running   0          13m     100.114.41.131   tkg-sandbox-control-plane-p276c     <none>           <none>
```

Cluster APIã®ãƒªã‚½ãƒ¼ã‚¹ã‚‚è¦‹ã¦ã¿ã¾ã™ã€‚

```
$ kubectl get cluster -A
NAMESPACE    NAME          PHASE
tkg-system   tkg-sandbox   Provisioned

$ kubectl get machinedeployment -A
NAMESPACE    NAME               PHASE     REPLICAS   AVAILABLE   READY
tkg-system   tkg-sandbox-md-0   Running   1          1           1

$ kubectl get machineset -A       
NAMESPACE    NAME                          REPLICAS   AVAILABLE   READY
tkg-system   tkg-sandbox-md-0-6498fc8bd4   1          1           1

$ kubectl get machine -A   
NAMESPACE    NAME                                PROVIDERID                                       PHASE
tkg-system   tkg-sandbox-control-plane-p276c     vsphere://420680e6-f224-b13a-e260-1ee80cd2eacd   Running
tkg-system   tkg-sandbox-md-0-6498fc8bd4-k59gr   vsphere://42063041-93a6-b855-b234-97e1dda67190   Running

$ kubectl get machinehealthcheck -A
NAMESPACE    NAME          MAXUNHEALTHY   EXPECTEDMACHINES   CURRENTHEALTHY
tkg-system   tkg-sandbox   100%           1                  1

$ kubectl get kubeadmcontrolplane -A
NAMESPACE    NAME                        READY   INITIALIZED   REPLICAS   READY REPLICAS   UPDATED REPLICAS   UNAVAILABLE REPLICAS
tkg-system   tkg-sandbox-control-plane   true    true          1          1                1                  

$ kubectl get kubeadmconfigtemplate -A
NAMESPACE    NAME               AGE
tkg-system   tkg-sandbox-md-0   9m59s
```

vSphereå®Ÿè£…ã®ãƒªã‚½ãƒ¼ã‚¹ã‚‚è¦‹ã¦ã¿ã¾ã™ã€‚

```
$ kubectl get vspherecluster -A 
NAMESPACE    NAME          AGE
tkg-system   tkg-sandbox   10m

$ kubectl get vspheremachine -A
NAMESPACE    NAME                              AGE
tkg-system   tkg-sandbox-control-plane-82ws2   10m
tkg-system   tkg-sandbox-worker-gg8cs          10m

$ kubectl get vspheremachinetemplate -A
NAMESPACE    NAME                        AGE
tkg-system   tkg-sandbox-control-plane   10m
tkg-system   tkg-sandbox-worker          10m
```

### Workload Clusterã‚’ä½œæˆ

å¯¾è±¡ã®Management Clusterã‚’`tkg set management-cluster`ã§æŒ‡å®šã—ã¾ã™ã€‚1ã¤ã—ã‹ãªã‘ã‚Œã°ä¸è¦ã§ã™ã€‚

```
$ tkg set management-cluster tkg-sandbox
The current management cluster context is switched to tkg-sandbox
```

å¯¾è±¡ã¨ãªã£ã¦ã„ã‚‹Management Clusterã‚’`tkg set management-cluster`ã§ç¢ºèªã§ãã¾ã™ã€‚`*`ãŒã¤ã„ã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãŒå¯¾è±¡ã§ã™ã€‚

```
$ tkg get management-cluster            
 MANAGEMENT-CLUSTER-NAME  CONTEXT-NAME                  
 tkg-sandbox *            tkg-sandbox-admin@tkg-sandbox 
```

`tkg create cluster`ã§Workload Clusterã‚’ä½œæˆã—ã¾ã™ã€‚

```
tkg create cluster demo --plan dev
```

æ¬¡ã®ã‚ˆã†ãªãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

```
Logs of the command execution can also be found at: /var/folders/76/vg4pyy253pbgwzncmx2mb1gh0000gq/T/tkg-20200922T200429948113328.log
Validating configuration...
Creating workload cluster 'demo'...
Waiting for cluster to be initialized...
Waiting for cluster nodes to be available...

Workload cluster 'demo' created
```

vSphereä¸Šã«ã¯æ¬¡ã®VMãŒä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/93875271-a7d03b80-fd0f-11ea-96c5-82b01371d271.png)

`tkg get cluster`ã§Workload Clusterã‚’ç¢ºèªã§ãã¾ã™ã€‚

```
$ tkg get cluster
 NAME  NAMESPACE  STATUS   CONTROLPLANE  WORKERS  KUBERNETES       
 demo  default    running  1/1           1/1      v1.18.6+vmware.1 
```

Cluster APIã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl get cluster -A
NAMESPACE    NAME          PHASE
default      demo          Provisioned
tkg-system   tkg-sandbox   Provisioned

$ kubectl get machinedeployment -A
NAMESPACE    NAME               PHASE     REPLICAS   AVAILABLE   READY
default      demo-md-0          Running   1          1           1
tkg-system   tkg-sandbox-md-0   Running   1          1           1

$ kubectl get machineset -A  
NAMESPACE    NAME                          REPLICAS   AVAILABLE   READY
default      demo-md-0-685646df5c          1          1           1
tkg-system   tkg-sandbox-md-0-6498fc8bd4   1          1           1

$ kubectl get machine -A   
NAMESPACE    NAME                                PROVIDERID                                       PHASE
default      demo-control-plane-wkngh            vsphere://42063032-21f6-9174-a095-f62516c4945b   Running
default      demo-md-0-685646df5c-gcb8n          vsphere://42063ac3-be34-04ce-0e9b-826065e65211   Running
tkg-system   tkg-sandbox-control-plane-p276c     vsphere://420680e6-f224-b13a-e260-1ee80cd2eacd   Running
tkg-system   tkg-sandbox-md-0-6498fc8bd4-k59gr   vsphere://42063041-93a6-b855-b234-97e1dda67190   Running

$ kubectl get machinehealthcheck -A
NAMESPACE    NAME          MAXUNHEALTHY   EXPECTEDMACHINES   CURRENTHEALTHY
default      demo          100%           1                  1
tkg-system   tkg-sandbox   100%           1                  1

$ kubectl get kubeadmcontrolplane -A
NAMESPACE    NAME                        READY   INITIALIZED   REPLICAS   READY REPLICAS   UPDATED REPLICAS   UNAVAILABLE REPLICAS
default      demo-control-plane          true    true          1          1                1                  
tkg-system   tkg-sandbox-control-plane   true    true          1          1                1  

$ kubectl get kubeadmconfigtemplate -A
NAMESPACE    NAME               AGE
default      demo-md-0          7m40s
tkg-system   tkg-sandbox-md-0   19m

$ kubectl get vspherecluster -A 
NAMESPACE    NAME          AGE
default      demo          8m5s
tkg-system   tkg-sandbox   19m

$ kubectl get vspheremachine -A
NAMESPACE    NAME                              AGE
default      demo-control-plane-xlzwf          7m21s
default      demo-worker-m7sqh                 8m13s
tkg-system   tkg-sandbox-control-plane-82ws2   19m
tkg-system   tkg-sandbox-worker-gg8cs          19m

$ kubectl get vspheremachinetemplate -A
NAMESPACE    NAME                        AGE
default      demo-control-plane          8m27s
default      demo-worker                 8m26s
tkg-system   tkg-sandbox-control-plane   20m
tkg-system   tkg-sandbox-worker          20m
```

Workload Cluster `demo`ã®configã‚’å–å¾—ã—ã¦ã€Current Contextã«è¨­å®šã—ã¾ã™ã€‚

```
tkg get credentials demo
kubectl config use-context demo-admin@demo
```

Workload Cluster `demo`ã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl cluster-info
Kubernetes master is running at https://10.213.173.211:6443
KubeDNS is running at https://10.213.173.211:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$ kubectl get node -o wide
NAME                         STATUS   ROLES    AGE     VERSION            INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                 KERNEL-VERSION   CONTAINER-RUNTIME
demo-control-plane-wkngh     Ready    master   7m10s   v1.18.6+vmware.1   10.213.173.212   10.213.173.212   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-gcb8n   Ready    <none>   4m58s   v1.18.6+vmware.1   10.213.173.213   10.213.173.213   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4

$ kubectl get pod -A -o wide
NAMESPACE     NAME                                               READY   STATUS    RESTARTS   AGE     IP               NODE                         NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-54fd4b48dd-57gdb           1/1     Running   0          5m44s   100.119.74.193   demo-control-plane-wkngh     <none>           <none>
kube-system   calico-node-7dq4r                                  1/1     Running   0          5m45s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   calico-node-tzn8b                                  1/1     Running   0          5m16s   10.213.173.213   demo-md-0-685646df5c-gcb8n   <none>           <none>
kube-system   coredns-5cf78cdcc-6vsmv                            1/1     Running   0          6m32s   100.119.74.197   demo-control-plane-wkngh     <none>           <none>
kube-system   coredns-5cf78cdcc-9w69b                            1/1     Running   0          6m32s   100.119.74.195   demo-control-plane-wkngh     <none>           <none>
kube-system   etcd-demo-control-plane-wkngh                      1/1     Running   0          6m15s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-apiserver-demo-control-plane-wkngh            1/1     Running   0          5m50s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-controller-manager-demo-control-plane-wkngh   1/1     Running   0          6m8s    10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-proxy-dwpvl                                   1/1     Running   0          5m16s   10.213.173.213   demo-md-0-685646df5c-gcb8n   <none>           <none>
kube-system   kube-proxy-z9qqj                                   1/1     Running   0          6m32s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-scheduler-demo-control-plane-wkngh            1/1     Running   0          6m11s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-cloud-controller-manager-9m4rv             1/1     Running   0          6m32s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-csi-controller-8c9b98f7f-w6pb8             5/5     Running   0          6m32s   100.119.74.194   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-csi-node-ct7sn                             3/3     Running   0          6m32s   100.119.74.196   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-csi-node-vk5jx                             3/3     Running   0          5m16s   100.111.20.65    demo-md-0-685646df5c-gcb8n   <none>           <none>
```

### MetalLBã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

TKG on vSphereã®å ´åˆã€Workload Clusterã«å¯¾ã™ã‚‹LoadBalancerãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã“ã“ã§ã¯å‹•ä½œæ¤œè¨¼ç”¨ã«[MetalLB](https://metallb.universe.tf)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

TKGã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã§DHCPã®ç¯„å›²å¤–ã§ç©ºã„ã¦ã„ã‚‹IPã®ç¯„å›²ã‚’æ¬¡ã®`METALLB_START_IP`ã¨`METALLB_END_IP`ã«è¨­å®šã—ã¾ã™ã€‚Workload Clusteræ¯ã«ç•°ãªã‚‹ç¯„å›²ã‚’æŒ‡å®šã—ãªã„ã¨ã„ã‘ãªã„ã“ã¨ã«æ°—ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦MetalLBã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```
METALLB_START_IP=10.213.173.50
METALLB_END_IP=10.213.173.70

mkdir -p metallb
wget -O metallb/namespace.yaml https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
wget -O metallb/metallb.yaml https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)" --dry-run=client -o yaml > metallb/secret.yaml

cat > metallb/configmap.yaml << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - ${METALLB_START_IP}-${METALLB_END_IP}
EOF
kubectl apply -f metallb/namespace.yaml -f metallb/metallb.yaml -f metallb/secret.yaml -f metallb/configmap.yaml
```

Podã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl get pod -n metallb-system
NAME                          READY   STATUS    RESTARTS   AGE
controller-57f648cb96-9mh99   1/1     Running   0          82s
speaker-5qxk5                 1/1     Running   0          83s
speaker-kgmw8                 1/1     Running   0          83s
```

å‹•ä½œç¢ºèªç”¨ã«ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚

```
kubectl create deployment demo --image=making/hello-world --dry-run=client -o=yaml > /tmp/deployment.yaml
echo --- >> /tmp/deployment.yaml
kubectl create service loadbalancer demo --tcp=80:8080 --dry-run=client -o=yaml >> /tmp/deployment.yaml
kubectl apply -f /tmp/deployment.yaml
```

Podã¨Serviceã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl get pod,svc -l app=demo
NAME                        READY   STATUS    RESTARTS   AGE
pod/demo-6996c67686-l847v   1/1     Running   0          25s

NAME           TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE
service/demo   LoadBalancer   100.70.73.202   10.213.173.50   80:31150/TCP   25s
```

ã‚¢ãƒ—ãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚

```
$ curl http://10.213.173.50                
Hello World!

$ curl http://10.213.173.50/actuator/health
{"status":"UP","groups":["liveness","readiness"]}
```

ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
kubectl delete -f /tmp/deployment.yaml
```

### StorageClassã®è¨­å®š

æ¬¡ã®ã‚ˆã†ã«Workload Clusterä½œæˆç›´å¾Œã¯`StorageClass`ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

```
$ kubectl get storageclass
No resources found in default namespace.
```

Menuã‹ã‚‰"Tags & Custom Attributes"ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93879364-8aeb3680-fd16-11ea-9e2f-22c6ac0038cc.png)

"CATEGORIES"ã‚’é¸æŠã—ã€"NEW"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€

* "Category Name:"ã«å¯¾ã—ã¦`k8s-storage`ã‚’å…¥åŠ›
* "Associatable Object Types:"ã«å¯¾ã—ã¦`Datastore`, `Datastore Cluster`ã‚’é¸æŠ

ã—ã¦OKã‚’ã‚¯ãƒªãƒƒã‚¯ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93883121-1adfaf00-fd1c-11ea-9438-c4d6414d6abc.png)

`k8s-storage`ã‚«ãƒ†ã‚´ãƒªãŒä½œæˆã•ã‚Œã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/93883484-92154300-fd1c-11ea-9cc1-dc2cf22624c5.png)

æ¬¡ã«"TAGS"ã‚’é¸æŠã—ã€"NEW"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€

* "Name:"ã«å¯¾ã—ã¦`k8s-storage`ã‚’å…¥åŠ›
* "Category:"ã«å¯¾ã—ã¦`k8s-storage`ã‚’é¸æŠ

ã—ã¦OKã‚’ã‚¯ãƒªãƒƒã‚¯ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93883522-a5c0a980-fd1c-11ea-825b-91694e20fe11.png)

`k8s-storage`ã‚¿ã‚°ãŒä½œæˆã•ã‚Œã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/93883574-b5d88900-fd1c-11ea-8374-791b0f2fe5ba.png)

Menuã‹ã‚‰"Storage"ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

å¯¾è±¡ã®Datastoreã®Summaryå†…ã®Tagsã‹ã‚‰"Assign Tag"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€`k8s-storage`ã«ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã€"ASSIGN"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93884335-baea0800-fd1d-11ea-9735-48603057e829.png)

Summaryå†…ã®Tagsã«`k8s-storage`ã‚¿ã‚°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/93884374-c806f700-fd1d-11ea-841e-d0b874ae1d16.png)

Menuã‹ã‚‰"Policy and Profiles"ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93883610-c426a500-fd1c-11ea-99be-1300130c0cf7.png)

"VM Storage Profiles"ã‚’é¸æŠã—ã€"Create VM Storage Policy"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

"Name:"ã«å¯¾ã—ã¦`k8s Storage Policy`ã‚’å…¥åŠ›ã—ã¦"NEXT"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93883736-eddfcc00-fd1c-11ea-92e2-e2da76acc144.png)

"Enable tag based placement rules"ã«å¯¾ã—ã¦ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã€"NEXT"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93883767-f89a6100-fd1c-11ea-8c8d-dc8af0053665.png)

"Tag category"ã«å¯¾ã—ã¦`k8s-storage`ã‚’å…¥åŠ›ã—ã€"BROWSE TAGS"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦`k8s-storage`ã‚’é¸æŠã—ã€"NEXT"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93883811-064fe680-fd1d-11ea-8643-a6a87ad128d3.png)

Tagã‚’è¨­å®šã—ãŸDatastoreãŒãƒªã‚¹ãƒˆã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€"NEXT"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93884474-e79e1f80-fd1d-11ea-8350-f302a4823235.png)

è¨­å®šå†…å®¹ã‚’ç¢ºèªã—ã¦"FINISH"ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

![image](https://user-images.githubusercontent.com/106908/93884503-eff65a80-fd1d-11ea-8af2-246955fbdbfc.png)

VM Storage Policiesä¸€è¦§ã«`k8s Storage Policy`ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/93884539-fedd0d00-fd1d-11ea-8f83-cd806265445a.png)

æ¬¡ã«ã€ã“ã®Storage Policyã‚’è¨­å®šã—ãŸ`StorageClass`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```
cat <<EOF > storageclass.yml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: standard
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: csi.vsphere.vmware.com
parameters:
  storagepolicyname: "k8s Storage Policy"
  fstype: ext3
allowVolumeExpansion: true
EOF
kubectl apply -f storageclass.yml
```

`StorageClass`ä¸€è¦§ã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl get storageclass
NAME                 PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard (default)   csi.vsphere.vmware.com   Delete          Immediate           true                   6s
```

å‹•ä½œç¢ºèªã®ãŸã‚ã€æ¬¡ã®`PersistentVolumeClaim`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```
cat <<EOF > /tmp/pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
EOF
kubectl apply -f /tmp/pvc.yml
```

`PersistentVolumeClaim`åŠã³`PersistentVolume`ã‚’ç¢ºèªã—ã€`STATUS`ãŒ`Bound`ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```
$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE
persistentvolume/pvc-77a75ab7-38cd-4a3c-a534-cbbe79d0abd1   1Gi        RWO            Delete           Bound    default/task-pv-claim   standard                5s

NAME                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/task-pv-claim   Bound    pvc-77a75ab7-38cd-4a3c-a534-cbbe79d0abd1   1Gi        RWO            standard       7s
```

vCenterä¸Šã§ã“ã®Volumeã‚’ç¢ºèªã§ãã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/93887011-18338880-fd21-11ea-857c-33314f74228e.png)

ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
kubectl delete -f /tmp/pvc.yml
```

### Workload Clusterã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ

`tkg scale cluster`ã‚³ãƒãƒ³ãƒ‰ã§Workload Clusterã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã§ãã¾ã™ã€‚

```
tkg scale cluster demo -w 3
```

WorkerãŒ3å°ã«å¢—ãˆãŸã“ã¨ãŒç¢ºèªã§ãã¾ã™ã€‚

```
$ kubectl get node -o wide
NAME                         STATUS   ROLES    AGE    VERSION            INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                 KERNEL-VERSION   CONTAINER-RUNTIME
demo-control-plane-wkngh     Ready    master   132m   v1.18.6+vmware.1   10.213.173.212   10.213.173.212   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-gcb8n   Ready    <none>   130m   v1.18.6+vmware.1   10.213.173.213   10.213.173.213   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-hksh9   Ready    <none>   70s    v1.18.6+vmware.1   10.213.173.215   10.213.173.215   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-pslwf   Ready    <none>   70s    v1.18.6+vmware.1   10.213.173.214   10.213.173.214   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
```

### Workload Clusterã‚’å‰Šé™¤

`demo` Workload Clusterã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
tkg delete cluster demo -y
```

`tkg get cluster`ã‚’ç¢ºèªã™ã‚‹ã¨STATUSãŒ`deleting`ã«ãªã‚Šã¾ã™ã€‚

```
$ tkg get cluster
 NAME  NAMESPACE  STATUS    CONTROLPLANE  WORKERS  KUBERNETES 
 demo  default    deleting 
```

ã—ã°ã‚‰ãã™ã‚‹ã¨`tkg get cluster`ã®çµæœã‹ã‚‰`demo`ãŒæ¶ˆãˆã¾ã™ã€‚

```
$ tkg get cluster 
 NAME  NAMESPACE  STATUS  CONTROLPLANE  WORKERS  KUBERNETES 
```

### Management Clusterã®å‰Šé™¤

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§Management Clusterã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
source tkg-env.sh
tkg delete management-cluster tkg-sandbox -y
```

Management Clusterã®å‰Šé™¤ã¯KindãŒä½¿ã‚ã‚Œã¾ã™ã€‚

---

TKGã§vSphere 6.7ä¸Šã«Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸã€‚

`tkg`ã‚³ãƒãƒ³ãƒ‰(+ Cluster API)ã«ã‚ˆã‚Šä¸€è²«ã—ãŸæ‰‹æ³•ã§AWSã§ã‚‚vSphereã§ã‚‚åŒã˜ã‚ˆã†ã«ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆãƒ»ç®¡ç†ã§ãã‚‹ãŸã‚ã€
ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰ã§Kubernetesã‚’ç®¡ç†ã—ã¦ã„ã‚‹å ´åˆã¯ç‰¹ã«æœ‰ç”¨ã ã¨æ€ã„ã¾ã™ã€‚

Azureã¨GCPã¯ä»Šå¾Œå¯¾å¿œã™ã‚‹ã‚ˆã†ã§ã™ã€‚
