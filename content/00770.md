---
title: llama-cpp-pythonを使ってローカルLLMでテキスト生成とOpenAI互換サーバーを立てる
tags: ["Python", "llama.cpp", "OpenAI", "Machine Learning", "MPS"]
categories: ["AI", "LLM", "llama.cpp"]
---


[llama.cpp](https://github.com/ggerganov/llama.cpp)はMetaのLLaMAモデルのC/C++版でラップトップで。
元々はApple SiliconのMacで動かす用だったようですが、LinuxやWindowsで大丈夫みたいです。Apple SiliconのMetalが利用できます。

このllama.cppのPythonバインディングである[llama-cpp-python](https://github.com/abetlen/llama-cpp-python)を試してみます。
llama-cpp-pythonは付加機能としてOpenAI互換のサーバーを立てることができます。


試した環境はこちらです

<img width="682" src="https://github.com/making/blog.ik.am/assets/106908/f8c9ef7e-bc21-47f7-86ca-0c0752ddd9c4">


**目次**
<!-- toc -->

### llama-cpp-pythonのインストール

まずはvenvを作成します。

```
python3 -m venv .venv
source .venv/bin/activate
```

llama-cpp-pythonのインストールします。serverも一緒にインストールします。

```
CMAKE_ARGS="-DLLAMA_METAL=on" pip install --force-reinstall --no-cache-dir 'llama-cpp-python[server]'
```

> ℹ️ Apple SiliconのMac上でエラーが出る場合は、 https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md のセットアップを試してください。

### Modelのダウンロード

Hugging Face上のモデルをダウンロードします。GGUF形式である必要があります。


自分が試したのは以下の二つです。

* https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF
* https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF


```
sudo mkdir -p /opt/models
sudo chown -R $USER /opt/models
```

ここでは https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf を使用します。

```
cd /opt/models/
wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf
```

簡単なテキストを生成するPythonコードを書きます。


```python
cat <<EOF > demo.py
from llama_cpp import Llama
llm = Llama(model_path="/opt/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf", n_gpu_layers=1)
output = llm("""
[INST] Give me a joke. [/INST]
""")
print(output['choices'][0]['text'])
EOF
```

実行します。何かジョークを言ってくれました。

```
$ python3 demo.py 2> /dev/null
Why don't scientists trust atoms? 
Because they make up everything!
```

### OpenAI互換サーバーの起動

以下のコマンドでサーバーを立ち上げます。

```
python3 -m llama_cpp.server --model /opt/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf --n_gpu_layers 1
```

http://localhost:8000/docs

```
curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
   "messages": [
      {"role": "user", "content": "Give me a joke."}
   ]
 }' | jq .
```


```
{
  "id": "chatcmpl-93596a2a-6933-4c40-82a4-2fed63bc5dda",
  "object": "chat.completion",
  "created": 1700207715,
  "model": "/opt/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
  "choices": [
    {
      "index": 0,
      "message": {
        "content": " Why don't scientists trust atoms? Because they make up everything!",
        "role": "assistant"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 17,
    "completion_tokens": 14,
    "total_tokens": 31
  }
}
```


```
curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
   "model": "gpt-3.5-turbo",
   "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Who won the world series in 2020?"},
      {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
      {"role": "user", "content": "Where was it played?"}
   ]
 }' | jq .
```


```
{
  "id": "chatcmpl-94d6afac-f029-4284-a8e3-7e01e2ab569e",
  "object": "chat.completion",
  "created": 1700207737,
  "model": "gpt-3.5-turbo",
  "choices": [
    {
      "index": 0,
      "message": {
        "content": " The 2020 World Series was played in Arlington, Texas. It was the first time that the World Series has been played in October since 1988, and it is the first time it has been held in October in the American League Championship Series format.",
        "role": "assistant"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 68,
    "completion_tokens": 57,
    "total_tokens": 125
  }
}
```