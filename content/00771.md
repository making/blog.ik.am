---
title: java-llama.cppを使ってローカルLLMでテキスト生成する
summary: この記事では、java-llama.cpp 4.1でローカルLLMをJavaに組み込む手順を紹介します。
tags: ["Java", "llama.cpp", "Machine Learning", "MPS"]
categories: ["AI", "LLM", "llama.cpp"]
date: 2023-11-20T03:46:09Z
updated: 2025-06-30T02:16:33Z
---

> [!INFO] 2025-06-30 java-llama.cpp 4.1の内容に更新しました。

[llama.cppのJavaバインディング](https://github.com/kherud/java-llama.cpp)を試します。


次の依存ライブラリを追加

```xml
<dependency>
    <groupId>de.kherud</groupId>
    <artifactId>llama</artifactId>
    <version>4.1.0</version>
</dependency>
```

次のコードを記述

```java
package com.example;

import de.kherud.llama.InferenceParameters;
import de.kherud.llama.LlamaModel;
import de.kherud.llama.LlamaOutput;
import de.kherud.llama.ModelParameters;
import de.kherud.llama.args.MiroStat;

public class Main {

	public static void main(String... args) {
		ModelParameters modelParams = new ModelParameters().setModel(System.getProperty("user.home")
				+ "/Library/Caches/llama.cpp/ggml-org_gemma-3-27b-it-GGUF_gemma-3-27b-it-Q4_K_M.gguf");
		InferenceParameters inferParams = new InferenceParameters("""
				「Attention is all you need.」がもたらした影響を端的に噛み砕いて教えてください。
				""").setTemperature(0f);
		;
		try (LlamaModel model = new LlamaModel(modelParams)) {
			Iterable<LlamaOutput> outputs = model.generate(inferParams);
			for (LlamaOutput output : outputs) {
				System.out.print(output);
			}
		}
	}

}
```

実行

```bash
$ mvn -q compile exec:java -Dexec.mainClass=com.example.Main 2> /dev/null
/de/kherud/llama/Mac/aarch64
Extracted 'libjllama.dylib' to '/var/folders/6m/jnyp8461431g15qf217pmth40000gn/T/libjllama.dylib'
「Attention is all you need.」は、2017年に発表された論文で、自然言語処理の分野に革命をもたらしました。その影響を噛み砕いて説明します。

**従来の課題：**

*   **RNNやLSTMの限界:** それまでの自然言語処理の主流は、RNN（Recurrent Neural Network）やLSTM（Long Short-Term Memory）といった再帰型ニューラルネットワークでした。これらは、文章を順番に処理するため、長い文章になると情報の損失が起こりやすく、並列処理が難しいという課題がありました。
*   **翻訳の難しさ:** 機械翻訳では、文章の構造を理解し、単語間の関係性を正確に捉える必要がありましたが、RNNやLSTMでは限界がありました。

**「Attention is all you need.」の提案：**

*   **Transformerの登場:** この論文では、RNNやLSTMを使わずに、**Attentionメカニズム**のみで構成された新しいモデル「Transformer」を提案しました。
*   **Attentionメカニズムとは:** 文章中の各単語が、他の単語とどれくらい関係があるかを数値化する仕組みです。これにより、文章全体の関係性を捉え、重要な部分に焦点を当てることができます。
*   **並列処理が可能に:** Transformerは、文章全体を一度に処理できるため、RNNやLSTMよりも高速に学習・推論ができます。

**具体的な影響：**

*   **自然言語処理の性能向上:** Transformerは、機械翻訳、文章生成、質問応答など、様々な自然言語処理タスクで、それまでの最高性能を大幅に上回る結果を出しました。
*   **BERT、GPTなどの大規模言語モデルの登場:** Transformerを基盤としたBERTやGPTといった大規模言語モデルが登場し、自然言語処理の可能性を大きく広げました。これらのモデルは、人間が書いた文章と区別がつかないほど自然な文章を生成したり、複雑な質問に答えたりすることができます。
*   **画像認識、音声認識への応用:** Transformerは、自然言語処理だけでなく、画像認識や音声認識といった他の分野にも応用され、目覚ましい成果を上げています。

**まとめ：**

「Attention is all you need.」は、自然言語処理の分野に**並列処理**と**長距離依存性**という2つの重要な要素をもたらし、その後のAI技術の発展に大きな影響を与えました。Transformerは、現在でも多くのAIモデルの基盤として使われており、私たちの生活を大く変えつつあります。

より詳しく知りたい場合は、以下のキーワードで検索してみてください。

*   Transformer
*   Attentionメカニズム
*   BERT
*   GPT
*   自然言語処理
*   大規模言語モデル
```

JavaアプリにローカルLLMを組み込むのが簡単になりそうです。
