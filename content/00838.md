---
title: llama-cppでDeepSeek-R1-Distill-Qwen-32B-Japaneseを動かしてSpring AIからアクセスする
tags: ["llama.cpp", "OpenAI", "Machine Learning", "MPS", "DeepSeek R1", "Spring AI"]
categories: ["AI", "LLM", "llama.cpp"]
---

**目次**
<!-- toc -->

サイバーエージェントが公開した[DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)(DeepSeek-R1を日本語向けに蒸留しなおしたモデル)をllama-cppで動かし、
OpenAI API経由でアクセスします。


実行環境はこちらです
<img width="878" src="https://github.com/user-attachments/assets/c4a4076a-8a2f-4bf7-a8f1-7dfb005837d9" />

### llama-cppのインストール

```
brew install llama.cpp
```

### OpenAI API Serverの起動

実際にはGGUFフォーマット変換版の[mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)を使用します。初回はモデルがダウンロードされます。

```
llama-server -hf mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf --port 8000
```

```
build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.2.0
system info: n_threads = 12, n_threads_batch = 12, total_threads = 16

system_info: n_threads = 12 (n_threads_batch = 12) / 16 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 

main: HTTP server is listening, hostname: 127.0.0.1, port: 8000, http threads: 15
main: loading model
srv    load_model: loading model '/Users/toshiaki/Library/Caches/llama.cpp/mmnga_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf'
common_download_file: previous metadata file found /Users/toshiaki/Library/Caches/llama.cpp/mmnga_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf.json: {"etag":"\"a3e66d7c746c3f4bf60dd74c668d5a38-1000\"","lastModified":"Mon, 27 Jan 2025 11:33:22 GMT","url":"https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf"}
curl_perform_with_retry: Trying to download from https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf (attempt 1 of 3)...
llama_model_load_from_file_impl: using device Metal (Apple M4 Max) - 98303 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 771 tensors from /Users/toshiaki/Library/Caches/llama.cpp/mmnga_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B Japanese
llama_model_loader: - kv   3:                           general.finetune str              = Japanese
llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   5:                         general.size_label str              = 32B
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Deepseek Ai
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["japanese", "qwen2", "text-generation"]
llama_model_loader: - kv  12:                          general.languages arr[str,1]       = ["ja"]
llama_model_loader: - kv  13:                          qwen2.block_count u32              = 64
llama_model_loader: - kv  14:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv  15:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv  16:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv  17:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  18:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  19:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  20:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - kv  33:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 5120
print_info: n_layer          = 64
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 27648
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 32B
print_info: model params     = 32.76 B
print_info: general.name     = DeepSeek R1 Distill Qwen 32B Japanese
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 148848 'ÄĬ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
```

簡易UIにアクセスします。

<img width="1024" alt="image" src="https://github.com/user-attachments/assets/66f598f0-0fa0-4b07-85d5-7ef992e7e52d">



次のプロンプトをUIに貼り付けてSubmitします。

```
A～Dの中で3人は正直者で、1人が嘘つきです。嘘つきはだれでしょう？

A：Dは嘘つきだ。
B：私は嘘をついていない。
C：Aは嘘をついていない。
D：Bは嘘つきだ。
```

![Image](https://github.com/user-attachments/assets/e159060e-50af-47ce-99b2-d84e24617eb3)

論理的な思考経過が流れてきて最終的に答えが返ります。Dが正解です。

UIで試した内容もOpenAI APIに対して、curlでもアクセスします。

```bash
curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "A～Dの中で3人は正直者で、1人が嘘つきです。嘘つきはだれでしょう？\n\nA：Dは嘘つきだ。\nB：私は嘘をついていない。\nC：Aは嘘をついていない。\nD：Bは嘘つきだ。\n"}
    ]
  }' | jq .
```
次のJSONが返ります。`<think>...</think>`の部分が思考過程。
```json
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "<think>\nまず、問題の状況を整理します。AからDまで4人中、3人は正直者で1人は嘘つきです。それぞれの発言を基に、誰が嘘つきかを特定する必要があります。\n\n各人の発言を確認します：\n- Aは「Dは嘘つきだ」と言っています。\n- Bは「私は嘘をついていない」と言っています。\n- Cは「Aは嘘をついていない」と言っています。\n- Dは「Bは嘘つきだ」と言っています。\n\nまず、正直者と嘘つきの関係を考えます。正直者は真実を述べ、嘘つきは偽りを述べます。したがって、各人の発言が真実か偽かによって、自分が正直者か嘘つきかが決まります。\n\n次に、各人が正直者か嘘つきかの場合を仮定して検証していきます。4人中1人が嘘つきなので、その1人を仮定し、その場合の矛盾がないかどうかを確認します。\n\nまず、Aが嘘つきの場合を考えてみます。\nAが嘘つきなら、Aの発言「Dは嘘つきだ」は偽なので、Dは正直者です。\nCは「Aは嘘をついていない」と言っていますが、Aが嘘つきならCの発言は偽なので、Cは嘘つきになります。しかし、嘘つきは1人しかいないので、この場合、AとCが嘘つきになる矛盾が生じます。よって、Aが嘘つきの可能性は否定されます。\n\n次に、Bが嘘つきの場合を仮定します。\nBが嘘つきなら、Bの発言「私は嘘をついていない」は偽なので、Bは嘘つきです。\nBが嘘つきの場合、Dの発言「Bは嘘つきだ」は真実なので、Dは正直者です。\nAの発言「Dは嘘つきだ」は偽なので、Aは嘘つきになりますが、嘘つきはBしかいないはずが、Aも嘘つきになる矛盾が生じます。よって、Bが嘘つきの可能性も否定されます。\n\n次に、Cが嘘つきの場合を考えます。\nCが嘘つきなら、Cの発言「Aは嘘をついていない」は偽なので、Aは嘘つきです。\nAが嘘つきなら、Aの発言「Dは嘘つきだ」は偽なので、Dは正直者です。\nDが正直者なら、Dの発言「Bは嘘つきだ」は真実なので、Bは嘘つきになります。しかし、嘘つきはCとBの2人になるため、矛盾が生じます。よって、Cが嘘つきの可能性も否定されます。\n\n最後に、Dが嘘つきの場合を検討します。\nDが嘘つきなら、Dの発言「Bは嘘つきだ」は偽なので、Bは正直者です。\nBが正直者なら、Bの発言「私は嘘をついていない」は真実なので、Bは正直者です。\nAの発言「Dは嘘つきだ」は真実なので、Aは正直者です。\nCの発言「Aは嘘をついていない」は真実なので、Cは正直者です。\nこの場合、嘘つきはDのみで、他の3人（A、B、C）が正直者となり、条件を満たします。したがって、Dが嘘つきです。\n</think>\n\n**答え：D**\n\n**解説：**\n1. **各人の発言を整理**  \n   - A: Dは嘘つき  \n   - B: 自分は正直  \n   - C: Aは正直  \n   - D: Bは嘘つき  \n\n2. **矛盾のない仮定を探す**  \n   - **Aが嘘つき**  \n     → Dは正直（Aの発言が偽）  \n     → Cの発言「Aは正直」が偽 → Cも嘘つき → 矛盾（嘘つきが2人）  \n\n   - **Bが嘘つき**  \n     → Bの発言が偽 → Bは嘘つき  \n     → Dの発言「Bは嘘つき」が真 → Dは正直  \n     → Aの発言「Dは嘘つき」が偽 → Aも嘘つき → 矛盾（嘘つきが2人）  \n\n   - **Cが嘘つき**  \n     → Cの発言「Aは正直」が偽 → Aは嘘つき  \n     → Aの発言「Dは嘘つき」が偽 → Dは正直  \n     → Dの発言「Bは嘘つき」が真 → Bは嘘つき → 矛盾（嘘つきが2人）  \n\n   - **Dが嘘つき**  \n     → Dの発言「Bは嘘つき」が偽 → Bは正直  \n     → Bの発言「自分は正直」が真 → Bは正直  \n     → Aの発言「Dは嘘つき」が真 → Aは正直  \n     → Cの発言「Aは正直」が真 → Cは正直  \n     → **嘘つきはDのみ**（条件を満たす）  \n\n**結論：Dが嘘つき**",
        "role": "assistant"
      }
    }
  ],
  "created": 1738243717,
  "model": "gpt-3.5-turbo",
  "system_fingerprint": "b4589-eb7cf15a",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 1207,
    "prompt_tokens": 62,
    "total_tokens": 1269
  },
  "id": "chatcmpl-GyOOzEH08sntwpx8tio4HFDrqpFC7hTS",
  "timings": {
    "prompt_n": 62,
    "prompt_ms": 401.295,
    "prompt_per_token_ms": 6.4725,
    "prompt_per_second": 154.4998068752414,
    "predicted_n": 1207,
    "predicted_ms": 70933.127,
    "predicted_per_token_ms": 58.76812510356255,
    "predicted_per_second": 17.01602694041671
  }
}
```

#### Spring AIでアクセス

[Spring AI](https://docs.spring.io/spring-ai/reference/index.html)を使ったアプリからアクセスしてみます。
OpenAI互換なので、Spring AIの[OpenAI用のChat Client](https://docs.spring.io/spring-ai/reference/api/clients/openai-chat.html)が利用できます。

サンプルアプリはこちらです。<br>
https://github.com/making/hello-spring-ai

```
git clone https://github.com/making/hello-spring-ai
cd hello-spring-ai
./mvnw clean package -DskipTests=true
java -jar target/hello-spring-ai-0.0.1-SNAPSHOT.jar --spring.ai.openai.base-url=http://localhost:8000 --spring.ai.openai.api-key=dummy
```

プロンプトをURLエンコードして`prompt`リクエストパラメータに設定してリクエストを送ります。

```
$ curl "http://localhost:8080?prompt=A%EF%BD%9ED%E3%81%AE%E4%B8%AD%E3%81%A73%E4%BA%BA%E3%81%AF%E6%AD%A3%E7%9B%B4%E8%80%85%E3%81%A7%E3%80%811%E4%BA%BA%E3%81%8C%E5%98%98%E3%81%A4%E3%81%8D%E3%81%A7%E3%81%99%E3%80%82%E5%98%98%E3%81%A4%E3%81%8D%E3%81%AF%E3%81%A0%E3%82%8C%E3%81%A7%E3%81%97%E3%82%87%E3%81%86%EF%BC%9F%5Cn%5CnA%EF%BC%9AD%E3%81%AF%E5%98%98%E3%81%A4%E3%81%8D%E3%81%A0%E3%80%82%5CnB%EF%BC%9A%E7%A7%81%E3%81%AF%E5%98%98%E3%82%92%E3%81%A4%E3%81%84%E3%81%A6%E3%81%84%E3%81%AA%E3%81%84%E3%80%82%5CnC%EF%BC%9AA%E3%81%AF%E5%98%98%E3%82%92%E3%81%A4%E3%81%84%E3%81%A6%E3%81%84%E3%81%AA%E3%81%84%E3%80%82%5CnD%EF%BC%9AB%E3%81%AF%E5%98%98%E3%81%A4%E3%81%8D%E3%81%A0%E3%80%82"

<think>
まず、問題を整理します。4人（A、B、C、D）のうち3人は正直者で1人は嘘つきです。それぞれの発言から、誰が嘘つきかを特定する必要があります。

各人の発言を確認します：
- A：Dは嘘つきだ。
- B：私は嘘をついていない。
- C：Aは嘘をついていない。
- D：Bは嘘つきだ。

仮定を立てて検証します。まず、各人が嘘つきの場合を考えてみます。

1. **Aが嘘つきの場合**：
   - Aの発言「Dは嘘つきだ」が偽なので、Dは正直者。
   - Dの発言「Bは嘘つきだ」が真なので、Bは嘘つき。
   - しかし、この場合、AとBが嘘つきとなり、2人になるため矛盾。よってAは正直者。

2. **Bが嘘つきの場合**：
   - Bの発言「私は嘘をついていない」が偽なので、Bは嘘つき。
   - Bが嘘つきなら、Dの発言「Bは嘘つきだ」が真で、Dは正直者。
   - Aの発言「Dは嘘つきだ」は偽になるが、Dは正直なので、Aは嘘つき。しかし、Aが嘘つきならCの発言「Aは嘘をついていない」は偽となり、Cも嘘つき。するとBとAとCが嘘つきとなり、3人になり矛盾。よってBは正直者。

3. **Cが嘘つきの場合**：
   - Cの発言「Aは嘘をついていない」が偽なので、Aは嘘つき。
   - Aが嘘つきなら、Aの発言「Dは嘘つきだ」は偽で、Dは正直者。
   - Dの発言「Bは嘘つきだ」が真なので、Bは嘘つき。
   - すると、CとAとBが嘘つきとなり、3人になり矛盾。よってCは正直者。

4. **Dが嘘つきの場合**：
   - Dの発言「Bは嘘つきだ」が偽なので、Bは正直者。
   - Bが正直なら、Bの発言「私は嘘をついていない」が真。
   - Cの発言「Aは嘘をついていない」が真なので、Aは正直者。
   - Aが正直なら、Aの発言「Dは嘘つきだ」が真で、Dは嘘つき。
   - この場合、Dだけが嘘つきで、A、B、Cは正直者。矛盾なし。

以上より、Dが嘘つきです。
</think>

**答え：D**

**解説：**
1. **仮定を立てる**  
   各人を「嘘つき」と仮定し、発言の整合性を確認します。

2. **Aが嘘つきの場合**  
   - Aの「Dは嘘つき」が偽 → Dは正直者。  
   - Dの「Bは嘘つき」が真 → Bは嘘つき。  
   - → **AとBが嘘つき**となり矛盾（最大1人）。**×**

3. **Bが嘘つきの場合**  
   - Bの「正直」が偽 → Bは嘘つき。  
   - Dの「Bは嘘つき」が真 → Dは正直者。  
   - Aの「Dは嘘つき」は偽 → Aは嘘つき。  
   - → **AとBが嘘つき**となり矛盾（最大1人）。**×**

4. **Cが嘘つきの場合**  
   - Cの「Aは正直」が偽 → Aは嘘つき。  
   - Aの「Dは嘘つき」は偽 → Dは正直者。  
   - Dの「Bは嘘つき」が真 → Bは嘘つき。  
   - → **AとBとCが嘘つき**となり矛盾（最大1人）。**×**

5. **Dが嘘つきの場合**  
   - Dの「Bは嘘つき」が偽 → Bは正直者。  
   - Bの「正直」が真 → Bは正直。  
   - Cの「Aは正直」が真 → Aは正直。  
   - Aの「Dは嘘つき」が真 → Dは嘘つき。  
   - → **Dだけが嘘つき**で整合性成立。**○**

**結論：Dが嘘つきです。**
```