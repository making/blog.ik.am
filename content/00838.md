---
title: llama.cppでDeepSeek-R1-Distill-Qwen-32B-Japaneseを動かしてSpring AIからアクセスする
tags: ["llama.cpp", "OpenAI", "Machine Learning", "MPS", "DeepSeek R1", "Spring AI"]
categories: ["AI", "LLM", "llama.cpp"]
---

**目次**
<!-- toc -->

サイバーエージェントが公開した[DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)(DeepSeek-R1を日本語向けに蒸留しなおしたモデル)をllama-cppで動かし、 OpenAI API経由でアクセスします。


実行環境はこちらです
<img width="878" src="https://github.com/user-attachments/assets/c4a4076a-8a2f-4bf7-a8f1-7dfb005837d9" />

### llama-cppのインストール

```
brew install llama.cpp
```

### OpenAI API Serverの起動

実際にはGGUFフォーマット変換版の[mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)を使用します。初回はモデルがダウンロードされます。

```
llama-server -hf mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf --port 8000
```

```
build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.2.0
system info: n_threads = 12, n_threads_batch = 12, total_threads = 16

system_info: n_threads = 12 (n_threads_batch = 12) / 16 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 

main: HTTP server is listening, hostname: 127.0.0.1, port: 8000, http threads: 15
main: loading model
srv    load_model: loading model '/Users/toshiaki/Library/Caches/llama.cpp/mmnga_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf'
common_download_file: previous metadata file found /Users/toshiaki/Library/Caches/llama.cpp/mmnga_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf.json: {"etag":"\"a3e66d7c746c3f4bf60dd74c668d5a38-1000\"","lastModified":"Mon, 27 Jan 2025 11:33:22 GMT","url":"https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf"}
curl_perform_with_retry: Trying to download from https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf (attempt 1 of 3)...
llama_model_load_from_file_impl: using device Metal (Apple M4 Max) - 98303 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 771 tensors from /Users/toshiaki/Library/Caches/llama.cpp/mmnga_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf_cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B Japanese
llama_model_loader: - kv   3:                           general.finetune str              = Japanese
llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   5:                         general.size_label str              = 32B
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Deepseek Ai
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["japanese", "qwen2", "text-generation"]
llama_model_loader: - kv  12:                          general.languages arr[str,1]       = ["ja"]
llama_model_loader: - kv  13:                          qwen2.block_count u32              = 64
llama_model_loader: - kv  14:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv  15:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv  16:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv  17:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  18:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  19:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  20:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - kv  33:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 5120
print_info: n_layer          = 64
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 27648
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 32B
print_info: model params     = 32.76 B
print_info: general.name     = DeepSeek R1 Distill Qwen 32B Japanese
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 148848 'ÄĬ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
```

簡易UIにアクセスします。

<img width="1024" alt="image" src="https://github.com/user-attachments/assets/66f598f0-0fa0-4b07-85d5-7ef992e7e52d">



次のプロンプトをUIに貼り付けてSubmitします。

```
A～Dの中で3人は正直者で、1人が嘘つきです。嘘つきはだれでしょう？

A：Dは嘘つきだ。
B：私は嘘をついていない。
C：Aは嘘をついていない。
D：Bは嘘つきだ。
```

![Image](https://github.com/user-attachments/assets/e159060e-50af-47ce-99b2-d84e24617eb3)

論理的な思考経過が流れてきて最終的に答えが返ります。Dが正解です。

UIで試した内容もOpenAI APIに対して、curlでもアクセスします。

```bash
curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "A～Dの中で3人は正直者で、1人が嘘つきです。嘘つきはだれでしょう？\n\nA：Dは嘘つきだ。\nB：私は嘘をついていない。\nC：Aは嘘をついていない。\nD：Bは嘘つきだ。\n"}
    ]
  }' | jq .
```
次のJSONが返ります。`<think>...</think>`の部分が思考過程。
```json
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "<think>\nまず、問題の状況を整理します。AからDまで4人中、3人は正直者で1人は嘘つきです。それぞれの発言を基に、誰が嘘つきかを特定する必要があります。\n\n各人の発言を確認します：\n- Aは「Dは嘘つきだ」と言っています。\n- Bは「私は嘘をついていない」と言っています。\n- Cは「Aは嘘をついていない」と言っています。\n- Dは「Bは嘘つきだ」と言っています。\n\nまず、正直者と嘘つきの関係を考えます。正直者は真実を述べ、嘘つきは偽りを述べます。したがって、各人の発言が真実か偽かによって、自分が正直者か嘘つきかが決まります。\n\n次に、各人が正直者か嘘つきかの場合を仮定して検証していきます。4人中1人が嘘つきなので、その1人を仮定し、その場合の矛盾がないかどうかを確認します。\n\nまず、Aが嘘つきの場合を考えてみます。\nAが嘘つきなら、Aの発言「Dは嘘つきだ」は偽なので、Dは正直者です。\nCは「Aは嘘をついていない」と言っていますが、Aが嘘つきならCの発言は偽なので、Cは嘘つきになります。しかし、嘘つきは1人しかいないので、この場合、AとCが嘘つきになる矛盾が生じます。よって、Aが嘘つきの可能性は否定されます。\n\n次に、Bが嘘つきの場合を仮定します。\nBが嘘つきなら、Bの発言「私は嘘をついていない」は偽なので、Bは嘘つきです。\nBが嘘つきの場合、Dの発言「Bは嘘つきだ」は真実なので、Dは正直者です。\nAの発言「Dは嘘つきだ」は偽なので、Aは嘘つきになりますが、嘘つきはBしかいないはずが、Aも嘘つきになる矛盾が生じます。よって、Bが嘘つきの可能性も否定されます。\n\n次に、Cが嘘つきの場合を考えます。\nCが嘘つきなら、Cの発言「Aは嘘をついていない」は偽なので、Aは嘘つきです。\nAが嘘つきなら、Aの発言「Dは嘘つきだ」は偽なので、Dは正直者です。\nDが正直者なら、Dの発言「Bは嘘つきだ」は真実なので、Bは嘘つきになります。しかし、嘘つきはCとBの2人になるため、矛盾が生じます。よって、Cが嘘つきの可能性も否定されます。\n\n最後に、Dが嘘つきの場合を検討します。\nDが嘘つきなら、Dの発言「Bは嘘つきだ」は偽なので、Bは正直者です。\nBが正直者なら、Bの発言「私は嘘をついていない」は真実なので、Bは正直者です。\nAの発言「Dは嘘つきだ」は真実なので、Aは正直者です。\nCの発言「Aは嘘をついていない」は真実なので、Cは正直者です。\nこの場合、嘘つきはDのみで、他の3人（A、B、C）が正直者となり、条件を満たします。したがって、Dが嘘つきです。\n</think>\n\n**答え：D**\n\n**解説：**\n1. **各人の発言を整理**  \n   - A: Dは嘘つき  \n   - B: 自分は正直  \n   - C: Aは正直  \n   - D: Bは嘘つき  \n\n2. **矛盾のない仮定を探す**  \n   - **Aが嘘つき**  \n     → Dは正直（Aの発言が偽）  \n     → Cの発言「Aは正直」が偽 → Cも嘘つき → 矛盾（嘘つきが2人）  \n\n   - **Bが嘘つき**  \n     → Bの発言が偽 → Bは嘘つき  \n     → Dの発言「Bは嘘つき」が真 → Dは正直  \n     → Aの発言「Dは嘘つき」が偽 → Aも嘘つき → 矛盾（嘘つきが2人）  \n\n   - **Cが嘘つき**  \n     → Cの発言「Aは正直」が偽 → Aは嘘つき  \n     → Aの発言「Dは嘘つき」が偽 → Dは正直  \n     → Dの発言「Bは嘘つき」が真 → Bは嘘つき → 矛盾（嘘つきが2人）  \n\n   - **Dが嘘つき**  \n     → Dの発言「Bは嘘つき」が偽 → Bは正直  \n     → Bの発言「自分は正直」が真 → Bは正直  \n     → Aの発言「Dは嘘つき」が真 → Aは正直  \n     → Cの発言「Aは正直」が真 → Cは正直  \n     → **嘘つきはDのみ**（条件を満たす）  \n\n**結論：Dが嘘つき**",
        "role": "assistant"
      }
    }
  ],
  "created": 1738243717,
  "model": "gpt-3.5-turbo",
  "system_fingerprint": "b4589-eb7cf15a",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 1207,
    "prompt_tokens": 62,
    "total_tokens": 1269
  },
  "id": "chatcmpl-GyOOzEH08sntwpx8tio4HFDrqpFC7hTS",
  "timings": {
    "prompt_n": 62,
    "prompt_ms": 401.295,
    "prompt_per_token_ms": 6.4725,
    "prompt_per_second": 154.4998068752414,
    "predicted_n": 1207,
    "predicted_ms": 70933.127,
    "predicted_per_token_ms": 58.76812510356255,
    "predicted_per_second": 17.01602694041671
  }
}
```

#### Spring AIでアクセス

[Spring AI](https://docs.spring.io/spring-ai/reference/index.html)を使ったアプリからアクセスしてみます。
OpenAI互換なので、Spring AIの[OpenAI用のChat Client](https://docs.spring.io/spring-ai/reference/api/clients/openai-chat.html)が利用できます。

サンプルアプリはこちらです。<br>
https://github.com/making/hello-spring-ai

```
git clone https://github.com/making/hello-spring-ai
cd hello-spring-ai
./mvnw clean package -DskipTests=true
java -jar target/hello-spring-ai-0.0.1-SNAPSHOT.jar --spring.ai.openai.base-url=http://localhost:8000 --spring.ai.openai.api-key=dummy
```

同じプロンプトを送ります。

```
$ curl http://localhost:8080 -H Content-Type: text/plain -d "A～Dの中で3人は正直者で、1人が嘘つきです。嘘つきはだれでしょう？\n\nA：Dは嘘つきだ。\nB：私は嘘をついていない。\nC：Aは嘘をついていない。\nD：Bは嘘つきだ。\n"

data:<think>
data:まず、問題文を整理します。

data:AからDまでの4人の中で、3人は正直者で1人は嘘つきです。

data:各人の発言から、誰が嘘つきかを特定する必要があります。
data:
data:各人の発言をリストアップします。
data:- A：Dは嘘つきだ。
data:- B：私は嘘をついていない。
data:- C：Aは嘘をついていない。
data:- D：Bは嘘つきだ。
data:
data:まず、各人が正直者か嘘つきかによって、発言の真偽を検証します。

data:正直者は真実を言うので、その発言は正しいはずです。

data:嘘つきは嘘をついているので、発言は偽りです。
data:
data:仮に各人が嘘つきである場合をそれぞれ検討します。
data:
data:1.

data: 偽りがAの場合：
data:   - Aは嘘つきなので、Dは正直者（Aの発言は嘘だから）。

data:Dは正直者なら、Dの発言「Bは嘘つきだ」は真実。

data:つまりBは嘘つき。

data:しかし、Bの発言は「私は嘘をついていない」なので、Bが嘘つきならこの発言は嘘で正しい。

data:しかし、この場合、AとBが嘘つきになるため、嘘つきが2人になってしまう。

data:矛盾するので、Aは嘘つきではない。
data:
data:2.

data: 偽りがBの場合：
data:   - Bは嘘つきなので、Bの発言「私は嘘をついていない」は嘘。

data:つまりBは嘘つき。

data:Bが嘘つきなら、Dの発言「Bは嘘つきだ」は真実。

data:Dは正直者。

data:すると、Dが正直者ならAの発言「Dは嘘つきだ」は嘘。

data:つまりAは嘘つき。

data:しかし、これだとBとAが嘘つきになるため、2人いて矛盾。

data:よってBは嘘つきではない。
data:
data:3.

data: 偽りがCの場合：
data:   - Cは嘘つきなので、Cの発言「Aは嘘をついていない」は嘘。

data:つまりAは嘘つき。

data:Aが嘘つきなら、Aの発言「Dは嘘つきだ」は嘘なので、Dは正直者。

data:Dが正直者なら、Dの発言「Bは嘘つきだ」は真実なのでBは嘘つき。

data:すると、CとBとAが嘘つきになり、3人になってしまう。

data:矛盾するので、Cは嘘つきではない。
data:
data:4.

data: 偽りがDの場合：
data:   - Dは嘘つきなので、Dの発言「Bは嘘つきだ」は嘘。

data:つまりBは正直者。

data:Bが正直者なら、Bの発言「私は嘘をついていない」は真実。

data:Cの発言「Aは嘘をついていない」が正しいかどうか。

data:Aが正直者なら、Aの発言「Dは嘘つきだ」は真実で正しい。

data:これで、Dだけが嘘つきで、A、B、Cが正直者になる。

data:この場合、矛盾が発生しないので、Dが嘘つきである可能性が高い。
data:
data:各ケースを検討した結果、Dが嘘つきの場合に矛盾が生じないことが確認できました。

data:したがって、嘘つきはDです。
data:</think>
data:
data:**解答：Dが嘘つきです。

data:**
data:
data:**根拠：**  
data:各人の発言を検証し、矛盾のない状況を導き出します。

data:  
data:- **Dが嘘つき**の場合：  
data:  - Dの発言「Bは嘘つきだ」は嘘→Bは正直者。

data:  
data:  - Bが正直者なら「私は嘘をついていない」が真実。

data:  
data:  - Cの発言「Aは嘘をついていない」が真実→Aも正直者。

data:  
data:  - Aが正直者なら「Dは嘘つきだ」が真実。

data:  
data:  - これでA・B・Cが正直者、Dのみ嘘つきとなり、矛盾なし。

data:  
data:
data:他のケース（A・B・Cが嘘つき）は、**2人以上の嘘つきが発生**するため不成立です。

data:  
data:よって、**Dが嘘つき**が唯一の解となります。
```