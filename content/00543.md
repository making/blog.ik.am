---
title: Tanzu Kubernetes Grid 1.1ã‚’AWSã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ¡ãƒ¢
tags: ["Kubernetes", "AWS", "BOSH", "TKG", "Tanzu", "Cluster API"]
categories: ["Dev", "CaaS", "Kubernetes", "TKG", "AWS"]
---

æœ¬è¨˜äº‹ã¯ã€åå‰å¤‰æ›´å¾Œåˆã®ãƒªãƒªãƒ¼ã‚¹ã§ã‚ã‚‹[Tanzu Kubernetes Grid (TKG)](https://docs.vmware.com/jp/VMware-Tanzu-Kubernetes-Grid/index.html) 1.1.2ã‚’AWSã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ¡ãƒ¢ã§ã™ã€‚


TKGã¯[Cluster API](https://cluster-api.sigs.k8s.io)(`clusterctl`)ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã®ã‚ˆã†ãªãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã§ã™ã€‚

Cluster APIã«ã¯

* Management Cluster
* Workload Cluster

ã¨ã„ã†2ç¨®é¡ã®Kubernetes ClusterãŒã‚ã‚Šã¾ã™ã€‚

Workload Clusterã¯Management Clusterã«ã‚ˆã£ã¦ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’ç®¡ç†ã•ã‚Œã¾ã™ã€‚<br>
ã¤ã¾ã‚Šã€Kubernetesã§ç®¡ç†ã•ã‚Œã‚‹Kubernetes Clusterã§ã™ã€‚

ã§ã¯ã€Kubernetesã‚’ç®¡ç†ã™ã‚‹Kubernetesã¯èª°ãŒç®¡ç†ã™ã‚‹ã®ã§ã—ã‚‡ã†ã‹?

...Kubernetesã§ã™ğŸ˜… <br>
TKGã§ã¯Management Clusterã¯[Kind](https://kind.sigs.k8s.io)ã«ã‚ˆã£ã¦bootstrapã•ã‚Œã¾ã™ã€‚

ã¤ã¾ã‚Š Kind -> Management Cluster -> Workload Clusterã¨ã„ã†è¦ªå­é–¢ä¿‚ã§Kubernetes ClusterãŒç®¡ç†ã•ã‚Œã‚‹ãŸã‚ã€
Cluster APIã¯æ¬¡ã®ã‚ˆã†ãªäº€ã®ãƒ­ã‚´ã«ãªã£ã¦ã„ã¾ã™ã€‚

<img src="https://user-images.githubusercontent.com/106908/90330477-e7588900-dfe7-11ea-86a6-91533b0b2fa1.png" width="200" />

Cluster APIã«ã¤ã„ã¦ã‚ˆã‚ŠçŸ¥ã‚ŠãŸã„å ´åˆã¯æ¬¡ã®KubeAcademyã®å‹•ç”»ã‚’è¦‹ã‚‹ã¨è‰¯ã„ã§ã™ã€‚

* https://kube.academy/lessons/bootstrapping-cluster-api-part-1-concepts-components-and-terminology
* https://kube.academy/lessons/bootstrapping-cluster-api-part-2-creating-a-cluster-on-aws-with-cluster-api

**Table of contents**
<!-- toc -->

### CLIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

[https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-112&productId=988&rPId=48121](https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-112&productId=988&rPId=48121)

ã‹ã‚‰

- clusterawsadm for Mac v0.5.4
- VMware Tanzu Kubernetes Grid CLI for Mac

ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¬¡ã®ã‚ˆã†ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

```
gunzip tkg-darwin-amd64-v1.1.2-vmware.1.gz
gunzip clusterawsadm-darwin-amd64-v0.5.4-vmware.1.gz

mv tkg-darwin-amd64-v1.1.2-vmware.1 /usr/local/bin/tkg
mv clusterawsadm-darwin-amd64-v0.5.4-vmware.1 /usr/local/bin/clusterawsadm

chmod +x /usr/local/bin/tkg
chmod +x /usr/local/bin/clusterawsadm
```

ãã®ä»–ã€DockerãŒå¿…è¦ã§ã™ã€‚


`clusterawsadm`ã¯[github](https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases)ã‹ã‚‰ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚

æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™ã€‚

```
$ tkg version          
Client:
	Version: v1.1.2
	Git commit: c1db5bed7bc95e2ba32cf683c50525cdff0f2396

$ clusterawsadm version
clusterawsadm version: &version.Info{Major:"", Minor:"", GitVersion:"", GitCommit:"", GitTreeState:"", BuildDate:"", GoVersion:"go1.13.6", AwsSdkVersion:"v1.31.3", Compiler:"gc", Platform:"darwin/amd64"}
```

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

```
tkg get management-cluster
```

æ¬¡ã®ã‚ˆã†ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒä½œæˆã—ã¾ã™ã€‚

```
$ find ~/.tkg 
/Users/toshiaki/.tkg
/Users/toshiaki/.tkg/bom
/Users/toshiaki/.tkg/bom/bom-1.1.2+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-tkg-1.0.0.yaml
/Users/toshiaki/.tkg/bom/bom-1.17.6+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-1.1.0+vmware.1.yaml
/Users/toshiaki/.tkg/providers
/Users/toshiaki/.tkg/providers/control-plane-kubeadm
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.6
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.6/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.5
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.5/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.3
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.3/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/providers.md5sum
/Users/toshiaki/.tkg/providers/infrastructure-vsphere
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/cluster-api
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.6
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.6/core-components.yaml
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.5
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.5/core-components.yaml
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.3
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.3/core-components.yaml
/Users/toshiaki/.tkg/providers/config.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.6
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.6/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.5
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.5/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.3
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.3/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0/cluster-template-prod.yaml
/Users/toshiaki/.tkg/config.yaml
```

### AWSã®IAMãƒªã‚½ãƒ¼ã‚¹ä½œæˆ

`clusterawsadm`ã§TKG(ã§ä½¿ã‚ã‚Œã‚‹Cluster API)ã§å¿…è¦ãªIAMãƒªã‚½ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚


```
export AWS_ACCESS_KEY_ID=****
export AWS_SECRET_ACCESS_KEY=****
export AWS_REGION=ap-northeast-1

$ clusterawsadm alpha bootstrap create-stack
Attempting to create CloudFormation stack cluster-api-provider-aws-sigs-k8s-io

Following resources are in the stack: 

Resource                  |Type                                                                                |Status
AWS::IAM::Group           |bootstrapper.cluster-api-provider-aws.sigs.k8s.io                                   |CREATE_COMPLETE
AWS::IAM::InstanceProfile |control-plane.cluster-api-provider-aws.sigs.k8s.io                                  |CREATE_COMPLETE
AWS::IAM::InstanceProfile |controllers.cluster-api-provider-aws.sigs.k8s.io                                    |CREATE_COMPLETE
AWS::IAM::InstanceProfile |nodes.cluster-api-provider-aws.sigs.k8s.io                                          |CREATE_COMPLETE
AWS::IAM::ManagedPolicy   |arn:aws:iam::120200614459:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io |CREATE_COMPLETE
AWS::IAM::ManagedPolicy   |arn:aws:iam::120200614459:policy/nodes.cluster-api-provider-aws.sigs.k8s.io         |CREATE_COMPLETE
AWS::IAM::ManagedPolicy   |arn:aws:iam::120200614459:policy/controllers.cluster-api-provider-aws.sigs.k8s.io   |CREATE_COMPLETE
AWS::IAM::Role            |control-plane.cluster-api-provider-aws.sigs.k8s.io                                  |CREATE_COMPLETE
AWS::IAM::Role            |controllers.cluster-api-provider-aws.sigs.k8s.io                                    |CREATE_COMPLETE
AWS::IAM::Role            |nodes.cluster-api-provider-aws.sigs.k8s.io                                          |CREATE_COMPLETE
AWS::IAM::User            |bootstrapper.cluster-api-provider-aws.sigs.k8s.io                                   |CREATE_COMPLETE
```

### Bastionç”¨SSH Keyã®ä½œæˆ

TKGã§ä½œæˆã•ã‚Œã‚‹Bastion Serverç”¨ã®SSH Keyã‚’ä½œæˆã—ã¾ã™ã€‚

```
export AWS_SSH_KEY_NAME=tkg-sandbox
aws ec2 create-key-pair --key-name tkg-sandbox --output json | jq .KeyMaterial -r > tkg-sandbox.pem
```

### Management Clusterã‚’AWSã«ä½œæˆ

TKGã§Workload Clusterã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ã€Management Clusterã‚’ä½œæˆã—ã¾ã™ã€‚

[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.1/vmware-tanzu-kubernetes-grid-11/GUID-install-tkg-aws-ui.html)ã§ã¯GUIã‚’ä½¿ã£ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ãŒã€CLIã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§å¿…è¦ãªç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚

```
export AWS_CREDENTIALS=$(aws iam create-access-key --user-name bootstrapper.cluster-api-provider-aws.sigs.k8s.io --output json)

cat <<EOF > tkg-env.sh
export AWS_ACCESS_KEY_ID=$(echo $AWS_CREDENTIALS | jq .AccessKey.AccessKeyId -r)
export AWS_SECRET_ACCESS_KEY=$(echo $AWS_CREDENTIALS | jq .AccessKey.SecretAccessKey -r)
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm alpha bootstrap encode-aws-credentials)
EOF

source tkg-env.sh
```

AWSã®æƒ…å ±ã‚’Management Clusterç”¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«(`~/.tkg/config.yaml`)ã«è¿½è¨˜ã—ã¾ã™ã€‚

```
cat <<EOF >> ~/.tkg/config.yaml
AWS_REGION: ${AWS_REGION}
AWS_NODE_AZ: ${AWS_REGION}a
AWS_PRIVATE_NODE_CIDR: 10.0.0.0/24
AWS_PUBLIC_NODE_CIDR: 10.0.1.0/24
AWS_PUBLIC_SUBNET_ID:
AWS_PRIVATE_SUBNET_ID:
AWS_SSH_KEY_NAME: ${AWS_SSH_KEY_NAME}
AWS_VPC_ID:
AWS_VPC_CIDR: 10.0.0.0/16
CLUSTER_CIDR: 100.96.0.0/11
CONTROL_PLANE_MACHINE_TYPE: t3.small
NODE_MACHINE_TYPE: t3.medium
EOF
```

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§Management Clusterã‚’ä½œæˆã—ã¾ã™ã€‚Control Planeã®å°æ•°ã¯`dev` planãŒ1å°ã€`prod`ãŒ3å°ã«ãªã‚Šã¾ã™ã€‚

```
tkg init --infrastructure aws --name tkg-sandbox --plan dev
```

> `tkg init`ã§Kindã‚¯ãƒ©ã‚¹ã‚¿ä½œæˆã€`clusterctl init`ã€`clusterclt config`ã€`kubectl apply`ç›¸å½“ã®å‡¦ç†ãŒè¡Œã‚ã‚Œã¾ã™ã€‚

æ¬¡ã®ã‚ˆã†ãªãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

```
Logs of the command execution can also be found at: /tmp/tkg-20200816T071656435755588.log

Validating the pre-requisites...

Setting up management cluster...
Validating configuration...
Using infrastructure provider aws:v0.5.4
Generating cluster configuration...
Setting up bootstrapper...
Bootstrapper created. Kubeconfig: /root/.kube-tkg/tmp/config_cYHcgTDa
Installing providers on bootstrapper...
Fetching providers
Installing cert-manager
Waiting for cert-manager to be available...
Installing Provider="cluster-api" Version="v0.3.6" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-aws" Version="v0.5.4" TargetNamespace="capa-system"
Start creating management cluster...
Saving management cluster kuebconfig into /root/.kube/config
Installing providers on management cluster...
Fetching providers
Installing cert-manager
Waiting for cert-manager to be available...
Installing Provider="cluster-api" Version="v0.3.6" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-aws" Version="v0.5.4" TargetNamespace="capa-system"
Waiting for the management cluster to get ready for move...
Moving all Cluster API objects from bootstrap cluster to management cluster...
Performing move...
Discovering Cluster API objects
Moving Cluster API objects Clusters=1
Creating objects in the target cluster
Deleting objects from the source cluster
Context set for management cluster tkg-sandbox as 'tkg-sandbox-admin@tkg-sandbox'.

Management cluster created!


You can now create your first workload cluster by running the following:

  tkg create cluster [name] --kubernetes-version=[version] --plan=[plan]
```

EC2ä¸Šã«ã¯æ¬¡ã®VMãŒä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/90330638-597d9d80-dfe9-11ea-9e68-dbf63abf3e30.png)

VPCã‚„NAT Gatewayã‚‚`config.yaml`ã«åˆã‚ã›ã¦ä½œæˆã•ã‚Œã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/90330792-88484380-dfea-11ea-865c-98775ea2ea2e.png)

![image](https://user-images.githubusercontent.com/106908/90330796-91391500-dfea-11ea-9463-5ed2e51543bd.png)

`tkg get management-cluster`ã‚³ãƒãƒ³ãƒ‰ã§Management Clusterä¸€è¦§ã‚’å–å¾—ã§ãã¾ã™ã€‚

```
$ tkg get management-cluster
 MANAGEMENT-CLUSTER-NAME  CONTEXT-NAME                  
 tkg-sandbox *            tkg-sandbox-admin@tkg-sandbox 
```

`tkg get cluster --include-management-cluster`ã‚³ãƒãƒ³ãƒ‰ã§Workload ClusteråŠã³Management Clusterä¸€è¦§ã‚’å–å¾—ã§ãã¾ã™ã€‚

```
$ tkg get cluster --include-management-cluster
   NAME         NAMESPACE   STATUS   CONTROLPLANE  WORKERS  KUBERNETES       
   tkg-sandbox  tkg-system  running  1/1           1/1      v1.18.3+vmware.1 
```

Management Clusterã«`kubectl`ã‚³ãƒãƒ³ãƒ‰ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã¿ã¾ã™ã€‚

```
$ kubectl config use-context tkg-sandbox-admin@tkg-sandbox
$ kubectl cluster-info
Kubernetes master is running at https://tkg-sandbox-apiserver-831815079.ap-northeast-1.elb.amazonaws.com:6443
KubeDNS is running at https://tkg-sandbox-apiserver-831815079.ap-northeast-1.elb.amazonaws.com:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
$ kubectl get node -o wide
NAME                                            STATUS   ROLES    AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-10-0-0-108.ap-northeast-1.compute.internal   Ready    master   93m   v1.18.3+vmware.1   10.0.0.108    <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
ip-10-0-0-28.ap-northeast-1.compute.internal    Ready    <none>   92m   v1.18.3+vmware.1   10.0.0.28     <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4

$ kubectl get pod -A -o wide
NAMESPACE                           NAME                                                                    READY   STATUS    RESTARTS   AGE   IP                NODE                                            NOMINATED NODE   READINESS GATES
capa-system                         capa-controller-manager-86cbf677bc-5j5zr                                2/2     Running   0          94m   100.119.52.4      ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-6857dfc668-gsw7p              2/2     Running   0          94m   100.100.223.199   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-85f4885cf5-59rkq          2/2     Running   0          94m   100.100.223.201   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
capi-system                         capi-controller-manager-5df8c8fb59-w4rbt                                2/2     Running   0          94m   100.100.223.197   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
capi-webhook-system                 capa-controller-manager-555b87ddbd-vs4m6                                2/2     Running   0          94m   100.100.223.202   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
capi-webhook-system                 capi-controller-manager-7d8d9b87b8-v2gpw                                2/2     Running   0          94m   100.100.223.196   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
capi-webhook-system                 capi-kubeadm-bootstrap-controller-manager-dff99d987-sg8dm               2/2     Running   0          94m   100.100.223.198   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
capi-webhook-system                 capi-kubeadm-control-plane-controller-manager-6cc995dd6c-7l5pk          2/2     Running   0          94m   100.100.223.200   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
cert-manager                        cert-manager-b56b4dc78-z9vcs                                            1/1     Running   0          96m   100.100.223.194   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
cert-manager                        cert-manager-cainjector-6b54f84d85-rz5sj                                1/1     Running   0          96m   100.100.223.193   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
cert-manager                        cert-manager-webhook-6fbc6d7449-k9zml                                   1/1     Running   0          96m   100.100.223.195   ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
kube-system                         calico-kube-controllers-54fd4b48dd-z6fdb                                1/1     Running   0          96m   100.119.52.2      ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         calico-node-5k4j6                                                       1/1     Running   0          95m   10.0.0.28         ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
kube-system                         calico-node-rm8xx                                                       1/1     Running   0          96m   10.0.0.108        ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         coredns-dbbffcb66-7hp22                                                 1/1     Running   0          96m   100.119.52.3      ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         coredns-dbbffcb66-q7zqj                                                 1/1     Running   0          96m   100.119.52.1      ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         etcd-ip-10-0-0-108.ap-northeast-1.compute.internal                      1/1     Running   0          96m   10.0.0.108        ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         kube-apiserver-ip-10-0-0-108.ap-northeast-1.compute.internal            1/1     Running   0          96m   10.0.0.108        ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         kube-controller-manager-ip-10-0-0-108.ap-northeast-1.compute.internal   1/1     Running   0          96m   10.0.0.108        ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         kube-proxy-879rs                                                        1/1     Running   0          95m   10.0.0.28         ip-10-0-0-28.ap-northeast-1.compute.internal    <none>           <none>
kube-system                         kube-proxy-h7ckg                                                        1/1     Running   0          96m   10.0.0.108        ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
kube-system                         kube-scheduler-ip-10-0-0-108.ap-northeast-1.compute.internal            1/1     Running   0          96m   10.0.0.108        ip-10-0-0-108.ap-northeast-1.compute.internal   <none>           <none>
```

Cluster APIã®ãƒªã‚½ãƒ¼ã‚¹ã‚‚è¦‹ã¦ã¿ã¾ã™ã€‚

```
$ kubectl get cluster -A
NAMESPACE    NAME          PHASE
tkg-system   tkg-sandbox   Provisioned

$ kubectl get machinedeployment -A
NAMESPACE    NAME               PHASE     REPLICAS   AVAILABLE   READY
tkg-system   tkg-sandbox-md-0   Running   1          1           1

$ kubectl get machineset -A       
NAMESPACE    NAME                          REPLICAS   AVAILABLE   READY
tkg-system   tkg-sandbox-md-0-75d49d8447   1          1           1

$ kubectl get machine -A   
NAMESPACE    NAME                                PROVIDERID                                   PHASE
tkg-system   tkg-sandbox-control-plane-8ldnq     aws:///ap-northeast-1a/i-042acfd83946a4758   Running
tkg-system   tkg-sandbox-md-0-75d49d8447-bqvd7   aws:///ap-northeast-1a/i-0d04c2727bf7727f7   Running

$ kubectl get machinehealthcheck -A
NAMESPACE    NAME          MAXUNHEALTHY   EXPECTEDMACHINES   CURRENTHEALTHY
tkg-system   tkg-sandbox   100%           1                  1

$ kubectl get kubeadmcontrolplane -A
NAMESPACE    NAME                        READY   INITIALIZED   REPLICAS   READY REPLICAS   UPDATED REPLICAS   UNAVAILABLE REPLICAS
tkg-system   tkg-sandbox-control-plane   true    true

$ kubectl get kubeadmconfigtemplate -A
NAMESPACE    NAME               AGE
tkg-system   tkg-sandbox-md-0   3h59m  
```

AWSå®Ÿè£…ã®ãƒªã‚½ãƒ¼ã‚¹ã‚‚è¦‹ã¦ã¿ã¾ã™ã€‚

```
$ kubectl get awscluster -A         
NAMESPACE    NAME          CLUSTER       READY   VPC                     BASTION IP
tkg-system   tkg-sandbox   tkg-sandbox   true    vpc-0b1166959b313d695   52.193.190.32

$ kubectl get awsmachine -A
NAMESPACE    NAME                              CLUSTER       STATE     READY   INSTANCEID                                   MACHINE
tkg-system   tkg-sandbox-control-plane-t8w2g   tkg-sandbox   running   true    aws:///ap-northeast-1a/i-042acfd83946a4758   tkg-sandbox-control-plane-8ldnq
tkg-system   tkg-sandbox-md-0-gdfk8            tkg-sandbox   running   true    aws:///ap-northeast-1a/i-0d04c2727bf7727f7   tkg-sandbox-md-0-75d49d8447-bqvd7

$ kubectl get awsmachinetemplate -A
NAMESPACE    NAME                        AGE
tkg-system   tkg-sandbox-control-plane   4h
tkg-system   tkg-sandbox-md-0            4h
```

### Workload Clusterã‚’ä½œæˆ

å¯¾è±¡ã®Management Clusterã‚’`tkg set management-cluster`ã§æŒ‡å®šã—ã¾ã™ã€‚1ã¤ã—ã‹ãªã‘ã‚Œã°ä¸è¦ã§ã™ã€‚

```
$ tkg set management-cluster tkg-sandbox
The current management cluster context is switched to tkg-sandbox
```

å¯¾è±¡ã¨ãªã£ã¦ã„ã‚‹Management Clusterã‚’`tkg set management-cluster`ã§ç¢ºèªã§ãã¾ã™ã€‚`*`ãŒã¤ã„ã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãŒå¯¾è±¡ã§ã™ã€‚

```
$ tkg get management-cluster            
 MANAGEMENT-CLUSTER-NAME  CONTEXT-NAME                  
 tkg-sandbox *            tkg-sandbox-admin@tkg-sandbox 
```

`tkg create cluster`ã§Workload Clusterã‚’ä½œæˆã—ã¾ã™ã€‚

```
tkg create cluster demo --plan dev
```

æ¬¡ã®ã‚ˆã†ãªãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

```
Logs of the command execution can also be found at: /var/folders/76/vg4pyy253pbgwzncmx2mb1gh0000gq/T/tkg-20200816T203809287331044.log
Validating configuration...
Creating workload cluster 'demo'...
Waiting for cluster to be initialized...
Waiting for cluster nodes to be available...

Workload cluster 'demo' created
```

EC2ä¸Šã«ã¯æ¬¡ã®VMãŒä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/90333568-03692400-e002-11ea-8eaf-723fe7d2fdd5.png)

Workload Clusterç”¨ã«ã‚‚å°‚ç”¨ã®VPCã‚„NAT GatewayãŒ`config.yaml`ã«åˆã‚ã›ã¦ä½œæˆã•ã‚Œã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/90333629-5511ae80-e002-11ea-86cc-a8a29ee82915.png)

![image](https://user-images.githubusercontent.com/106908/90333633-6955ab80-e002-11ea-8948-99b31169a6cf.png)

Workload Clusterã‚’Management Clusterã¨åŒã˜VPCã«ä½œæˆã™ã‚‹æ–¹æ³•ã¯å¾Œè¿°ã—ã¾ã™ã€‚

`tkg get cluster`ã§Workload Clusterã‚’ç¢ºèªã§ãã¾ã™ã€‚

```
$ tkg get cluster
 NAME  NAMESPACE  STATUS   CONTROLPLANE  WORKERS  KUBERNETES       
 demo  default    running  1/1           1/1      v1.18.3+vmware.1 
```

Cluster APIã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl get cluster -A
NAMESPACE    NAME          PHASE
default      demo          Provisioned
tkg-system   tkg-sandbox   Provisioned

$ kubectl get machinedeployment -A
NAMESPACE    NAME               PHASE     REPLICAS   AVAILABLE   READY
default      demo-md-0          Running   1          1           1
tkg-system   tkg-sandbox-md-0   Running   1          1           1

$ kubectl get machineset -A  
NAMESPACE    NAME                          REPLICAS   AVAILABLE   READY
default      demo-md-0-558dbbdfdc          1          1           1
tkg-system   tkg-sandbox-md-0-75d49d8447   1          1           1

$ kubectl get machine -A   
NAMESPACE    NAME                                PROVIDERID                                   PHASE
default      demo-control-plane-r4rkt            aws:///ap-northeast-1a/i-0e8d967473156836d   Running
default      demo-md-0-558dbbdfdc-xctfb          aws:///ap-northeast-1a/i-068d2836f5801eb0d   Running
tkg-system   tkg-sandbox-control-plane-8ldnq     aws:///ap-northeast-1a/i-042acfd83946a4758   Running
tkg-system   tkg-sandbox-md-0-75d49d8447-bqvd7   aws:///ap-northeast-1a/i-0d04c2727bf7727f7   Running

$ kubectl get machinehealthcheck -A
NAMESPACE    NAME          MAXUNHEALTHY   EXPECTEDMACHINES   CURRENTHEALTHY
default      demo          100%           1                  1
tkg-system   tkg-sandbox   100%           1                  1

$ kubectl get kubeadmcontrolplane -A
NAMESPACE    NAME                        READY   INITIALIZED   REPLICAS   READY REPLICAS   UPDATED REPLICAS   UNAVAILABLE REPLICAS
default      demo-control-plane          true    true          1          1                1                  
tkg-system   tkg-sandbox-control-plane   true    true          1          1                1                  

$ kubectl get kubeadmconfigtemplate -A
NAMESPACE    NAME               AGE
default      demo-md-0          21m
tkg-system   tkg-sandbox-md-0   4h29m

$ kubectl get awscluster -A 
NAMESPACE    NAME          CLUSTER       READY   VPC                     BASTION IP
default      demo          demo          true    vpc-03c82dac6a9aabf5a   52.69.53.60
tkg-system   tkg-sandbox   tkg-sandbox   true    vpc-0b1166959b313d695   52.193.190.32

$ kubectl get awsmachine -A
NAMESPACE    NAME                              CLUSTER       STATE     READY   INSTANCEID                                   MACHINE
default      demo-control-plane-n82mx          demo          running   true    aws:///ap-northeast-1a/i-0e8d967473156836d   demo-control-plane-r4rkt
default      demo-md-0-5488f                   demo          running   true    aws:///ap-northeast-1a/i-068d2836f5801eb0d   demo-md-0-558dbbdfdc-xctfb
tkg-system   tkg-sandbox-control-plane-t8w2g   tkg-sandbox   running   true    aws:///ap-northeast-1a/i-042acfd83946a4758   tkg-sandbox-control-plane-8ldnq
tkg-system   tkg-sandbox-md-0-gdfk8            tkg-sandbox   running   true    aws:///ap-northeast-1a/i-0d04c2727bf7727f7   tkg-sandbox-md-0-75d49d8447-bqvd7

$ kubectl get awsmachinetemplate -A
NAMESPACE    NAME                        AGE
default      demo-control-plane          21m
default      demo-md-0                   21m
tkg-system   tkg-sandbox-control-plane   4h29m
tkg-system   tkg-sandbox-md-0            4h29m
```

Workload Cluster `demo`ã®configã‚’å–å¾—ã—ã¦ã€Current Contextã«è¨­å®šã—ã¾ã™ã€‚

```
tkg get credentials demo
kubectl config use-context demo-admin@demo
```

Workload Cluster `demo`ã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl cluster-info
Kubernetes master is running at https://demo-apiserver-929202159.ap-northeast-1.elb.amazonaws.com:6443
KubeDNS is running at https://demo-apiserver-929202159.ap-northeast-1.elb.amazonaws.com:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$ kubectl get node -o wide
NAME                                            STATUS   ROLES    AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-10-0-0-176.ap-northeast-1.compute.internal   Ready    master   39m   v1.18.3+vmware.1   10.0.0.176    <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
ip-10-0-0-18.ap-northeast-1.compute.internal    Ready    <none>   38m   v1.18.3+vmware.1   10.0.0.18     <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4

$ kubectl get pod -A -o wide
NAMESPACE     NAME                                                                    READY   STATUS    RESTARTS   AGE   IP                NODE                                            NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-54fd4b48dd-nvbn8                                1/1     Running   0          40m   100.125.242.195   ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   calico-node-t8mls                                                       1/1     Running   0          40m   10.0.0.18         ip-10-0-0-18.ap-northeast-1.compute.internal    <none>           <none>
kube-system   calico-node-wttxn                                                       1/1     Running   0          40m   10.0.0.176        ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   coredns-dbbffcb66-2pg8v                                                 1/1     Running   0          41m   100.125.242.194   ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   coredns-dbbffcb66-76chs                                                 1/1     Running   0          41m   100.125.242.193   ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   etcd-ip-10-0-0-176.ap-northeast-1.compute.internal                      1/1     Running   0          41m   10.0.0.176        ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   kube-apiserver-ip-10-0-0-176.ap-northeast-1.compute.internal            1/1     Running   0          41m   10.0.0.176        ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   kube-controller-manager-ip-10-0-0-176.ap-northeast-1.compute.internal   1/1     Running   0          41m   10.0.0.176        ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   kube-proxy-g2bgf                                                        1/1     Running   0          40m   10.0.0.18         ip-10-0-0-18.ap-northeast-1.compute.internal    <none>           <none>
kube-system   kube-proxy-tvtf2                                                        1/1     Running   0          41m   10.0.0.176        ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
kube-system   kube-scheduler-ip-10-0-0-176.ap-northeast-1.compute.internal            1/1     Running   0          41m   10.0.0.176        ip-10-0-0-176.ap-northeast-1.compute.internal   <none>           <none>
```

å‹•ä½œç¢ºèªç”¨ã«ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚

```
kubectl create deployment demo --image=making/hello-cnb --dry-run -o=yaml > /tmp/deployment.yaml
echo --- >> /tmp/deployment.yaml
kubectl create service loadbalancer demo --tcp=80:8080 --dry-run -o=yaml >> /tmp/deployment.yaml
kubectl apply -f /tmp/deployment.yaml
```

Podã¨Serviceã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl get pod,svc -l app=demo
NAME                        READY   STATUS    RESTARTS   AGE
pod/demo-84b79888b4-df2zz   1/1     Running   0          91s

NAME           TYPE           CLUSTER-IP      EXTERNAL-IP                                                                   PORT(S)        AGE
service/demo   LoadBalancer   10.97.118.247   a8ee5042f3fea494b880d599f6f92267-379929389.ap-northeast-1.elb.amazonaws.com   80:30183/TCP   91s
```

ã‚¢ãƒ—ãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚

```
$ curl http://a8ee5042f3fea494b880d599f6f92267-379929389.ap-northeast-1.elb.amazonaws.com/actuator/health
{"status":"UP","groups":["liveness","readiness"]}
```

ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
kubectl delete -f /tmp/deployment.yaml
```

### Workload Clusterã‚’å‰Šé™¤

ã„ã£ãŸã‚“`demo` Workload Clusterã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
tkg delete cluster demo -y
```

`tkg get cluster`ã‚’ç¢ºèªã™ã‚‹ã¨STATUSãŒ`deleting`ã«ãªã‚Šã¾ã™ã€‚

```
$ tkg get cluster
 NAME  NAMESPACE  STATUS    CONTROLPLANE  WORKERS  KUBERNETES 
 demo  default    deleting 
```

ã—ã°ã‚‰ãã™ã‚‹ã¨`tkg get cluster`ã®çµæœã‹ã‚‰`demo`ãŒæ¶ˆãˆã¾ã™ã€‚

```
$ tkg get cluster 
 NAME  NAMESPACE  STATUS  CONTROLPLANE  WORKERS  KUBERNETES 
```

### Workload Clusterã‚’Management Clusterã¨åŒã˜VPC/Subnetã«ä½œæˆ

æ¬¡ã«Workload Clusterã‚’Management Clusterã¨åŒã˜VPC/Subnetã«ä½œæˆã—ã¾ã™ã€‚Bastion VMã‚„NAT Gatewayã‚’Management Clusterã¨å…±ç”¨ã§ãã¾ã™ã€‚

`~/.tkg/config.yaml`ã‚’ç·¨é›†ã—ã€æ¬¡ã®ç®‡æ‰€ã‚’è¨­å®šã—ã¾ã™ã€‚

```
# ... (ç•¥) ...
AWS_PUBLIC_SUBNET_ID: <Management Clusterã®Public Subnet ID>
AWS_PRIVATE_SUBNET_ID: <Management Clusterã®Private Subnet ID>
AWS_SSH_KEY_NAME: tkg-sandbox
AWS_VPC_ID: <Management Clusterã®VPC ID>
# ... (ç•¥) ...
```

æ¬¡ã®å›³ã®ä¾‹ã ã¨ã€

![image](https://user-images.githubusercontent.com/106908/90334624-f8b28d00-e009-11ea-8452-a205c69b91bd.png)

`~/.tkg/config.yaml`ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```
# ... (ç•¥) ...
AWS_PUBLIC_SUBNET_ID: subnet-06054c4866dafd2ad
AWS_PRIVATE_SUBNET_ID: subnet-01061ff8fbb3eb50f
AWS_SSH_KEY_NAME: tkg-sandbox
AWS_VPC_ID: vpc-0b1166959b313d695
# ... (ç•¥) ...
```

ã“ã®Configã‚’ä½¿ã£ã¦Workload Clusterã‚’ä½œæˆã—ã¾ã™ã€‚

```
tkg create cluster demo --plan dev
```

EC2ä¸Šã®VMã‚’ç¢ºèªã™ã‚‹ã¨Workload Clusterç”¨ã®BastionãŒã„ãªã„ã“ã¨ã‚’ç¢ºèªã§ãã¾ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/90335581-da9c5b00-e010-11ea-8471-5a901ceba646.png)

VPCã‚‚NAT Gatewayã‚‚ä¸€ã¤ã®ã¾ã¾ã§ã™ã€‚

![image](https://user-images.githubusercontent.com/106908/90335624-2b13b880-e011-11ea-8aef-6beb867cea13.png)

![image](https://user-images.githubusercontent.com/106908/90335609-11727100-e011-11ea-9fae-a8015724143b.png)


`AWSCluster`ã¨`AWSMachine`ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```
$ kubectl get awscluster -A
NAMESPACE    NAME          CLUSTER       READY   VPC                     BASTION IP
default      demo          demo          true    vpc-0b1166959b313d695   
tkg-system   tkg-sandbox   tkg-sandbox   true    vpc-0b1166959b313d695   52.193.190.32

$ kubectl get awsmachine -A
NAMESPACE    NAME                              CLUSTER       STATE     READY   INSTANCEID                                   MACHINE
default      demo-control-plane-ctcft          demo          running   true    aws:///ap-northeast-1a/i-09c433f3993b0ea8b   demo-control-plane-9jndn
default      demo-md-0-s6cqm                   demo          running   true    aws:///ap-northeast-1a/i-0750a5c8002f880c8   demo-md-0-558dbbdfdc-48fps
tkg-system   tkg-sandbox-control-plane-t8w2g   tkg-sandbox   running   true    aws:///ap-northeast-1a/i-042acfd83946a4758   tkg-sandbox-control-plane-8ldnq
tkg-system   tkg-sandbox-md-0-gdfk8            tkg-sandbox   running   true    aws:///ap-northeast-1a/i-0d04c2727bf7727f7   tkg-sandbox-md-0-75d49d8447-bqvd7
```

å…ˆã»ã©ã¨åŒã˜ã‚ˆã†ã«ã€Workload Cluster demoã®configã‚’å–å¾—ã—ã¦ã€Current Contextã«è¨­å®šã—ã¾ã™ã€‚

```
tkg get credentials demo
kubectl config use-context demo-admin@demo
```

Workload Cluster `demo`ã‚’ç¢ºèªã—ã¾ã™ã€‚

```
$ kubectl cluster-info
Kubernetes master is running at https://demo-apiserver-1921586626.ap-northeast-1.elb.amazonaws.com:6443
KubeDNS is running at https://demo-apiserver-1921586626.ap-northeast-1.elb.amazonaws.com:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

$ kubectl get node -o wide 
NAME                                            STATUS   ROLES    AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-10-0-0-116.ap-northeast-1.compute.internal   Ready    <none>   18m   v1.18.3+vmware.1   10.0.0.116    <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
ip-10-0-0-177.ap-northeast-1.compute.internal   Ready    master   20m   v1.18.3+vmware.1   10.0.0.177    <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
```

### Workload Clusterã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ

`tkg scale cluster`ã‚³ãƒãƒ³ãƒ‰ã§Workload Clusterã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã§ãã¾ã™ã€‚

```
tkg scale cluster demo -w 3
```

WorkerãŒ3å°ã«å¢—ãˆãŸã“ã¨ãŒç¢ºèªã§ãã¾ã™ã€‚

```
kubectl get node -o wide
NAME                                            STATUS   ROLES    AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-10-0-0-11.ap-northeast-1.compute.internal    Ready    <none>   54s   v1.18.3+vmware.1   10.0.0.11     <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
ip-10-0-0-116.ap-northeast-1.compute.internal   Ready    <none>   21m   v1.18.3+vmware.1   10.0.0.116    <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
ip-10-0-0-135.ap-northeast-1.compute.internal   Ready    <none>   39s   v1.18.3+vmware.1   10.0.0.135    <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
ip-10-0-0-177.ap-northeast-1.compute.internal   Ready    master   22m   v1.18.3+vmware.1   10.0.0.177    <none>        Amazon Linux 2   4.14.181-140.257.amzn2.x86_64   containerd://1.3.4
```

### Clusterã®å‰Šé™¤

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§Workload Clusterã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
tkg delete cluster demo -y
```

Workload ClusterãŒå‰Šé™¤ã•ã‚ŒãŸå¾Œã€Management Clusterã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```
source tkg-env.sh
tkg delete management-cluster tkg-sandbox -y
```

Management Clusterã®å‰Šé™¤ã¯KindãŒä½¿ã‚ã‚Œã¾ã™ã€‚

---

TKGã§AWSä¸Šã«Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸã€‚

`tkg`ã‚³ãƒãƒ³ãƒ‰(+ Cluster API)ã«ã‚ˆã‚Šä¸€è²«ã—ãŸæ‰‹æ³•ã§AWSã§ã‚‚vSphereã§ã‚‚åŒã˜ã‚ˆã†ã«ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆãƒ»ç®¡ç†ã§ãã‚‹ãŸã‚ã€
ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰ã§Kubernetesã‚’ç®¡ç†ã—ã¦ã„ã‚‹å ´åˆã¯ç‰¹ã«æœ‰ç”¨ã ã¨æ€ã„ã¾ã™ã€‚

Azureã¨GCPã¯ä»Šå¾Œå¯¾å¿œã™ã‚‹ã‚ˆã†ã§ã™ã€‚

æ¬¡ã¯vSphereã§è©¦ã—ã¾ã™ã€‚